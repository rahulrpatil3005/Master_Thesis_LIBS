{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4392714e-2e7c-4828-91aa-b8f81294591f",
   "metadata": {},
   "source": [
    "# IMPORTING THE LIBRARIES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d525889-7089-450b-8158-612b0ddafbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb12269-8ea9-48b9-bb59-b6a6fe3825b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.pyplot as plt1 \n",
    "\n",
    "import statistics\n",
    "import os\n",
    "\n",
    "###############################################\n",
    "from peakutils import indexes\n",
    "from peakutils import baseline\n",
    "from scipy.signal import find_peaks as fp\n",
    "from scipy.signal import savgol_filter \n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import svm\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "###############################################\n",
    "from bokeh.plotting import figure , show\n",
    "from pybaselines import whittaker as pl\n",
    "\n",
    "\n",
    "\n",
    "from bokeh.io import output_notebook\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdb69c6-3ad7-4ef0-9464-62b03f62397c",
   "metadata": {},
   "source": [
    "# DATA ARRANGEMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b58fa81-9ce3-4234-8b5a-b858bddc483d",
   "metadata": {},
   "source": [
    "In the main directory we can see that there are 8 subfolders. \n",
    "\n",
    "Each subfolder have almost 12 spectras per sample , the idea behind that would be , instead of having just one spectra per sample , and to just rely on one information , its always better have to multiple measurements per samples , and then this could be used for building the Calibration Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b23c0c-b3ac-4b59-924f-ef0d38a007a7",
   "metadata": {},
   "source": [
    "Instead of having 12 different csv per samples , its always good to have a single dataframe -> This new dataframe will have 1st column as wavelength , and 2nd -13th column as Intensities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0ecf88-6675-4703-b9b8-9f0d6835f6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(folder_path):\n",
    "    # List to store DataFrames for intensity columns\n",
    "    intensity_dfs = []\n",
    "\n",
    "    # List to store file names\n",
    "    file_names = []\n",
    "\n",
    "    # Get a list of .txt files in the folder\n",
    "    txt_files = [file_name for file_name in os.listdir(folder_path) if file_name.endswith('.txt')]\n",
    "\n",
    "    # Sort the .txt files based on their numerical order\n",
    "    # txt_files.sort(key=lambda x: int(re.search(r'_(\\d+)\\.txt', x).group(1)))\n",
    "\n",
    "    # Read the wavelength values from the first file\n",
    "    first_file_path = os.path.join(folder_path, txt_files[0])\n",
    "    wavelength_df = pd.read_csv(first_file_path, header=None, delimiter=';', usecols=[0], names=['wavelength'])\n",
    "\n",
    "    # Loop through each file in ascending order\n",
    "    for file_name in txt_files:\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        \n",
    "        # Read intensity values from each file into a DataFrame\n",
    "        intensity_df = pd.read_csv(file_path, header=None, delimiter=';', usecols=[1], names=['intensity'])\n",
    "        \n",
    "        # Store intensity DataFrame\n",
    "        intensity_dfs.append(intensity_df)\n",
    "        \n",
    "        # Store file name\n",
    "        file_names.append(os.path.splitext(file_name)[0])\n",
    "\n",
    "    # Concatenate intensity DataFrames\n",
    "    result_df = pd.concat(intensity_dfs, axis=1)\n",
    "\n",
    "    # Add the wavelength column to the result DataFrame\n",
    "    result_df = pd.concat([wavelength_df, result_df], axis=1)\n",
    "\n",
    "    # Rename the columns with file names\n",
    "    result_df.columns = ['wavelength'] + file_names\n",
    "\n",
    "    return result_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7b2157-acda-46f4-9d9a-226024715bc2",
   "metadata": {},
   "source": [
    "Loading the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a46d114-05e7-46d0-93ef-59e912c20da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE1_Raw_df = load_data('Batch_2/sample_1')\n",
    "###################################################\n",
    "SAMPLE2_Raw_df = load_data('Batch_2/sample_2')\n",
    "###################################################\n",
    "SAMPLE3_Raw_df = load_data('Batch_2/sample_3')\n",
    "###################################################\n",
    "SAMPLE4_Raw_df = load_data('Batch_2/sample_4')\n",
    "###################################################\n",
    "SAMPLE5_Raw_df = load_data('Batch_2/sample_5')\n",
    "###################################################\n",
    "SAMPLE6_Raw_df = load_data('Batch_2/sample_6')\n",
    "###################################################\n",
    "SAMPLE7_Raw_df = load_data('Batch_2/sample_7')\n",
    "####################################################\n",
    "SAMPLE8_Raw_df = load_data('Batch_2/sample_8')\n",
    "####################################################\n",
    "SAMPLE9_Raw_df = load_data('Batch_3/sample_9')\n",
    "####################################################\n",
    "SAMPLE10_Raw_df = load_data('Batch_3/sample_10')\n",
    "####################################################\n",
    "SAMPLE11_Raw_df = load_data('Batch_3/sample_11')\n",
    "####################################################\n",
    "SAMPLE12_Raw_df = load_data('Batch_3/sample_12')\n",
    "####################################################\n",
    "SAMPLE13_Raw_df = load_data('Batch_3/sample_13')\n",
    "####################################################\n",
    "SAMPLE14_Raw_df = load_data('Batch_3/sample_14')\n",
    "####################################################\n",
    "SAMPLE15_Raw_df = load_data('Batch_3/sample_15')\n",
    "####################################################\n",
    "SAMPLE16_Raw_df = load_data('Batch_3/sample_16')\n",
    "####################################################\n",
    "SAMPLE17_Raw_df = load_data('Batch_3/sample_17')\n",
    "####################################################\n",
    "\n",
    "SAMPLE1_Raw_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a420d66a-2223-404e-be46-96a4d4d93c40",
   "metadata": {},
   "source": [
    "# Peak Selection and Data Trimming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90a6244-3918-411b-ab77-652212e87e72",
   "metadata": {},
   "source": [
    "The dataframe  which we have is very big ,it could be trimmed now according to the wavelength , by adjusting two parameters \"Wavelength_Min\" , \"Wavelength_Max\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fbcf80-80b5-4f24-b0d3-2dc01cfbed35",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Getting the full wavelength\n",
    "Wavelength_Min = 230 #200.041162 \n",
    "Wavelength_Max = 540 #963.321041 \n",
    "\n",
    "\n",
    "SAMPLE1_Select_df = SAMPLE1_Raw_df[(SAMPLE1_Raw_df['wavelength'] >= Wavelength_Min) & (SAMPLE1_Raw_df['wavelength'] <= Wavelength_Max)]\n",
    "SAMPLE2_Select_df = SAMPLE2_Raw_df[(SAMPLE2_Raw_df['wavelength'] >= Wavelength_Min) & (SAMPLE2_Raw_df['wavelength'] <= Wavelength_Max)]\n",
    "SAMPLE3_Select_df = SAMPLE3_Raw_df[(SAMPLE3_Raw_df['wavelength'] >= Wavelength_Min) & (SAMPLE3_Raw_df['wavelength'] <= Wavelength_Max)]\n",
    "SAMPLE4_Select_df = SAMPLE4_Raw_df[(SAMPLE4_Raw_df['wavelength'] >= Wavelength_Min) & (SAMPLE4_Raw_df['wavelength'] <= Wavelength_Max)]\n",
    "SAMPLE5_Select_df = SAMPLE5_Raw_df[(SAMPLE5_Raw_df['wavelength'] >= Wavelength_Min) & (SAMPLE5_Raw_df['wavelength'] <= Wavelength_Max)]\n",
    "SAMPLE6_Select_df = SAMPLE6_Raw_df[(SAMPLE6_Raw_df['wavelength'] >= Wavelength_Min) & (SAMPLE6_Raw_df['wavelength'] <= Wavelength_Max)]\n",
    "SAMPLE7_Select_df = SAMPLE7_Raw_df[(SAMPLE7_Raw_df['wavelength'] >= Wavelength_Min) & (SAMPLE7_Raw_df['wavelength'] <= Wavelength_Max)]\n",
    "SAMPLE8_Select_df = SAMPLE8_Raw_df[(SAMPLE8_Raw_df['wavelength'] >= Wavelength_Min) & (SAMPLE8_Raw_df['wavelength'] <= Wavelength_Max)]\n",
    "SAMPLE9_Select_df = SAMPLE9_Raw_df[(SAMPLE9_Raw_df['wavelength'] >= Wavelength_Min) & (SAMPLE9_Raw_df['wavelength'] <= Wavelength_Max)]\n",
    "SAMPLE10_Select_df = SAMPLE10_Raw_df[(SAMPLE10_Raw_df['wavelength'] >= Wavelength_Min) & (SAMPLE10_Raw_df['wavelength'] <= Wavelength_Max)]\n",
    "SAMPLE11_Select_df = SAMPLE11_Raw_df[(SAMPLE11_Raw_df['wavelength'] >= Wavelength_Min) & (SAMPLE11_Raw_df['wavelength'] <= Wavelength_Max)]\n",
    "SAMPLE12_Select_df = SAMPLE12_Raw_df[(SAMPLE12_Raw_df['wavelength'] >= Wavelength_Min) & (SAMPLE12_Raw_df['wavelength'] <= Wavelength_Max)]\n",
    "SAMPLE13_Select_df = SAMPLE13_Raw_df[(SAMPLE13_Raw_df['wavelength'] >= Wavelength_Min) & (SAMPLE13_Raw_df['wavelength'] <= Wavelength_Max)]\n",
    "SAMPLE14_Select_df = SAMPLE14_Raw_df[(SAMPLE14_Raw_df['wavelength'] >= Wavelength_Min) & (SAMPLE14_Raw_df['wavelength'] <= Wavelength_Max)]\n",
    "SAMPLE15_Select_df = SAMPLE15_Raw_df[(SAMPLE15_Raw_df['wavelength'] >= Wavelength_Min) & (SAMPLE15_Raw_df['wavelength'] <= Wavelength_Max)]\n",
    "SAMPLE16_Select_df = SAMPLE16_Raw_df[(SAMPLE16_Raw_df['wavelength'] >= Wavelength_Min) & (SAMPLE16_Raw_df['wavelength'] <= Wavelength_Max)]\n",
    "SAMPLE17_Select_df = SAMPLE17_Raw_df[(SAMPLE17_Raw_df['wavelength'] >= Wavelength_Min) & (SAMPLE17_Raw_df['wavelength'] <= Wavelength_Max)]\n",
    "\n",
    "\n",
    "SAMPLE1_Select_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0c9fff-fb15-4539-9330-9ed39eecf07a",
   "metadata": {},
   "source": [
    "Lets plot a line plot ,to get a better picture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2063c4-4f62-4727-aae2-333fb4e7d4a8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Selected_df_Plot = figure(title = 'Selected Data Plot' , x_axis_label = 'Wavelength' , y_axis_label = 'Intensity')\n",
    "\n",
    "# Selected_df_Plot.line(SAMPLE1_Select_df.wavelength,SAMPLE1_Select_df.sample1_6 , line_width = 2, color =\"blue\" )\n",
    "# #####################################################################################################################\n",
    "# Selected_df_Plot.line(SAMPLE2_Select_df.wavelength,SAMPLE2_Select_df.sample2_6 , line_width = 2, color =\"orange\" )\n",
    "# ###############################################################################################################\n",
    "# Selected_df_Plot.line(SAMPLE3_Select_df.wavelength,SAMPLE3_Select_df.sample3_6 , line_width = 2, color =\"green\")\n",
    "# ######################################################################################################################\n",
    "# Selected_df_Plot.line(SAMPLE4_Select_df.wavelength,SAMPLE4_Select_df.sample4_6 , line_width = 2, color =\"red\")\n",
    "# ##############################################################################################################\n",
    "# Selected_df_Plot.line(SAMPLE5_Select_df.wavelength,SAMPLE5_Select_df.sample5_6 , line_width = 2, color =\"purple\")\n",
    "# ###############################################################################################################\n",
    "# Selected_df_Plot.line(SAMPLE6_Select_df.wavelength,SAMPLE6_Select_df.sample6_6 , line_width = 2, color =\"brown\")\n",
    "# ####################################################################################################################\n",
    "# Selected_df_Plot.line(SAMPLE7_Select_df.wavelength,SAMPLE7_Select_df.sample7_6 , line_width = 2, color =\"pink\")\n",
    "# ##################################################################################################################\n",
    "# Selected_df_Plot.line(SAMPLE8_Select_df.wavelength,SAMPLE8_Select_df.sample8_6 , line_width = 2, color =\"gray\")\n",
    "# ###################################################################################################################\n",
    "# Selected_df_Plot.line(SAMPLE9_Select_df.wavelength,SAMPLE9_Select_df.sample9_6 , line_width = 2, color =\"olive\")\n",
    "# ###################################################################################################################\n",
    "# Selected_df_Plot.line(SAMPLE10_Select_df.wavelength,SAMPLE10_Select_df.sample10_6 , line_width = 2, color =\"Cyan\")\n",
    "# ###################################################################################################################\n",
    "# Selected_df_Plot.line(SAMPLE11_Select_df.wavelength,SAMPLE11_Select_df.sample11_6 , line_width = 2, color =\"black\")\n",
    "# ###################################################################################################################\n",
    "# Selected_df_Plot.line(SAMPLE12_Select_df.wavelength,SAMPLE12_Select_df.sample12_6 , line_width = 2, color =\"tomato\")\n",
    "# ###################################################################################################################\n",
    "# Selected_df_Plot.line(SAMPLE13_Select_df.wavelength,SAMPLE13_Select_df.sample13_6 , line_width = 2, color =\"steelblue\")\n",
    "# ###################################################################################################################\n",
    "# Selected_df_Plot.line(SAMPLE14_Select_df.wavelength,SAMPLE14_Select_df.sample14_6 , line_width = 2, color =\"limegreen\")\n",
    "# ###################################################################################################################\n",
    "# Selected_df_Plot.line(SAMPLE15_Select_df.wavelength,SAMPLE15_Select_df.sample15_6 , line_width = 2, color =\"deeppink\")\n",
    "# ###################################################################################################################\n",
    "# Selected_df_Plot.line(SAMPLE16_Select_df.wavelength,SAMPLE16_Select_df.sample16_6 , line_width = 2, color =\"gold\")\n",
    "# ###################################################################################################################\n",
    "# Selected_df_Plot.line(SAMPLE17_Select_df.wavelength,SAMPLE17_Select_df.sample17_6 , line_width = 2, color =\"indigo\")\n",
    "# ###################################################################################################################\n",
    "\n",
    "# Selected_df_Plot.width = 900\n",
    "# Selected_df_Plot.height = 500\n",
    "# show(Selected_df_Plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32553d02-101b-463e-91a9-0bec5f82ba88",
   "metadata": {},
   "source": [
    "# Data Preprocessing of the Spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105566ee-f241-48a2-8c71-10d3c13bc5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_correction(df):\n",
    "    \"\"\"\n",
    "    Perform baseline correction on the intensity columns of the input DataFrame and create a new DataFrame with corrected values.\n",
    "    \n",
    "    Parameters:\n",
    "        df (DataFrame): Input DataFrame containing the wavelength and intensity columns.\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: New DataFrame with baseline-corrected intensity columns and the same wavelength column as the input DataFrame.\n",
    "    \"\"\"\n",
    "    # Copy the 'wavelength' column from the input DataFrame\n",
    "    new_df = pd.DataFrame({'wavelength': df['wavelength']})\n",
    "    \n",
    "    # Perform baseline correction for each intensity column and add them to the new DataFrame\n",
    "    for col in df.columns[1:]:  # Exclude the 'wavelength' column\n",
    "        baseline, _ = pl.asls(df[col])\n",
    "        corrected_values = df[col] - baseline\n",
    "        new_df[col] = corrected_values\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac0636c-323a-47ae-9af1-1f1c6443ac04",
   "metadata": {},
   "source": [
    "The above plot eventhough a spectra , is still a Raw Spectra , which still has lot of Artifects , before proceeding for the Univariate Calibration , its important to Pre Process the Raw Spectra accordingly. Various Pre Processing Techniques could be used here :- \n",
    "\n",
    "1) Baseline Correction - Very Very little  background radiation is still present in the spectra, which corresponds to the spectral baseline and imposes difficulties for quantitative elemental analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdddbd95-1f4e-4c7e-8d6e-6a5cc4f0a404",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE1_BaselineCorrected_df = baseline_correction(SAMPLE1_Select_df)\n",
    "SAMPLE2_BaselineCorrected_df = baseline_correction(SAMPLE2_Select_df)\n",
    "SAMPLE3_BaselineCorrected_df = baseline_correction(SAMPLE3_Select_df)\n",
    "SAMPLE4_BaselineCorrected_df = baseline_correction(SAMPLE4_Select_df)\n",
    "SAMPLE5_BaselineCorrected_df = baseline_correction(SAMPLE5_Select_df)\n",
    "SAMPLE6_BaselineCorrected_df = baseline_correction(SAMPLE6_Select_df)\n",
    "SAMPLE7_BaselineCorrected_df = baseline_correction(SAMPLE7_Select_df)\n",
    "SAMPLE8_BaselineCorrected_df = baseline_correction(SAMPLE8_Select_df)\n",
    "SAMPLE9_BaselineCorrected_df = baseline_correction(SAMPLE9_Select_df)\n",
    "SAMPLE10_BaselineCorrected_df = baseline_correction(SAMPLE10_Select_df)\n",
    "SAMPLE11_BaselineCorrected_df = baseline_correction(SAMPLE11_Select_df)\n",
    "SAMPLE12_BaselineCorrected_df = baseline_correction(SAMPLE12_Select_df)\n",
    "SAMPLE13_BaselineCorrected_df = baseline_correction(SAMPLE13_Select_df)\n",
    "SAMPLE14_BaselineCorrected_df = baseline_correction(SAMPLE14_Select_df)\n",
    "SAMPLE15_BaselineCorrected_df = baseline_correction(SAMPLE15_Select_df)\n",
    "SAMPLE16_BaselineCorrected_df = baseline_correction(SAMPLE16_Select_df)\n",
    "SAMPLE17_BaselineCorrected_df = baseline_correction(SAMPLE17_Select_df)\n",
    "\n",
    "print(SAMPLE1_BaselineCorrected_df)\n",
    "\n",
    "mean_intensity_1 = SAMPLE1_BaselineCorrected_df['sample1_1'].mean()\n",
    "std_intensity_1= SAMPLE1_BaselineCorrected_df['sample1_1'].std()\n",
    "\n",
    "print(mean_intensity_1)\n",
    "print(std_intensity_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3835fd-106c-49d1-aeb9-6308caa4b4e1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Baseline_Correction_Plot = figure(title = 'Baseline Correction' , x_axis_label = 'Wavelength' , y_axis_label = 'Intensity')\n",
    "\n",
    "# Baseline_Correction_Plot.line(SAMPLE4_Select_df.wavelength,SAMPLE4_Select_df.sample4_1 , line_width = 2, color =\"red\" )\n",
    "# Baseline_Correction_Plot.line(SAMPLE4_BaselineCorrected_df.wavelength,SAMPLE4_BaselineCorrected_df.sample4_1 , line_width = 2, color =\"green\" )\n",
    "\n",
    "\n",
    "# Baseline_Correction_Plot.width = 900\n",
    "# Baseline_Correction_Plot.height = 500\n",
    "# show(Baseline_Correction_Plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c6e970-ceb2-4d8f-8783-76534447c1c2",
   "metadata": {},
   "source": [
    "2) Normalization - Its usually noticed that , for a measurement of a similar sample ,there is a lot of scattering in the intensities , this typical artifect is called scattering.\n",
    "\n",
    "For Instance , you could see the plot above , Though this are the plots from the same sample SAMPLE1 , measured on 12 different Areas , its quite visible , for some  spectras , the peak heights or Intensities are not same.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7458b579-db93-4227-9d22-9408fc61d680",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def standard_normal_variate_normalization(df):\n",
    "#     # Selecting only the intensity columns (assuming the first column is wavelength)\n",
    "#     intensities = df.iloc[:, 1:]\n",
    "\n",
    "#     # Calculating mean and standard deviation for each column\n",
    "#     means = intensities.mean(axis=0)\n",
    "#     stds = intensities.std(axis=0)\n",
    "\n",
    "#     # Applying standard normal variate normalization column-wise\n",
    "#     normalized_intensities = (intensities - means) / stds\n",
    "\n",
    "#     # Combining the wavelength column with normalized intensities\n",
    "#     normalized_df = pd.concat([df.iloc[:, 0], normalized_intensities], axis=1)\n",
    "\n",
    "#     # Calculating mean and standard deviation for each normalized column\n",
    "#     normalized_means = normalized_intensities.mean(axis=0)\n",
    "#     normalized_stds = normalized_intensities.std(axis=0)\n",
    "\n",
    "#     return normalized_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f3937e-3ebd-400c-8b42-dc858abfd4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_normal_variate_normalization(df):\n",
    "    # Select columns containing intensities\n",
    "    intensity_cols = df.columns[1:]  #  columns 2 to 13 are intensities and are stored in intensity_cols\n",
    "\n",
    "    # SVN normalization\n",
    "    for col in intensity_cols:\n",
    "        mean_intensity = df[col].mean()\n",
    "        std_intensity = df[col].std()\n",
    "        df[col] = (df[col] - mean_intensity) / std_intensity\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be861edd-1e45-495d-9b6e-8f62e52d089a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def total_intensity_normalization(df):\n",
    "    # Selecting only the intensity columns (assuming the first column is wavelength)\n",
    "    intensities = df.iloc[:, 1:]  \n",
    "\n",
    "    # Initialize an empty DataFrame to store normalized intensities\n",
    "    normalized_intensities = pd.DataFrame()\n",
    "\n",
    "    # Iterate over each column (each spectrum)\n",
    "    for column in intensities.columns:\n",
    "        spectrum = intensities[column]\n",
    "        total_intensity = spectrum.sum()\n",
    "        \n",
    "        # Normalize the spectrum by dividing each intensity value by the total intensity\n",
    "        normalized_spectrum = spectrum / total_intensity\n",
    "        \n",
    "        # Append the normalized spectrum to the DataFrame of normalized intensities\n",
    "        normalized_intensities[column] = normalized_spectrum\n",
    "\n",
    "    # Combine the wavelength column with normalized intensities\n",
    "    normalized_df = pd.concat([df.iloc[:, 0], normalized_intensities], axis=1)\n",
    "\n",
    "    return normalized_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be72163b-e6ad-415c-a3d3-84d6ae5bc510",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def unit_norm_normalization(df):\n",
    "    # Selecting only the intensity columns (assuming the first column is wavelength)\n",
    "    intensities_1 = df.iloc[:, 1:]  \n",
    "    \n",
    "    # Initialize an empty DataFrame to store normalized intensities\n",
    "    normalized_intensities_1 = pd.DataFrame()\n",
    "\n",
    "    # Iterate over each column (each spectrum)\n",
    "    for column_1 in intensities_1.columns:\n",
    "        spectrum_1 = intensities_1[column_1]\n",
    "        \n",
    "        # Calculate the norm of the spectrum using a simple equation\n",
    "        spectrum_norm = np.sqrt(np.sum(spectrum_1 ** 2))\n",
    "        \n",
    "        # Normalize the spectrum by dividing each intensity value by its norm\n",
    "        normalized_spectrum_1 = spectrum_1 / spectrum_norm\n",
    "        \n",
    "        # Assign the normalized spectrum to the DataFrame of normalized intensities\n",
    "        normalized_intensities_1[column_1] = normalized_spectrum_1\n",
    "\n",
    "    # Combine the wavelength column with normalized intensities\n",
    "    normalized_df_1 = pd.concat([df.iloc[:, 0], normalized_intensities_1], axis=1)\n",
    "    \n",
    "    return normalized_df_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca606ff-3222-4233-a594-04c979996677",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def max_intensity_normalization(df):\n",
    "    # Selecting only the intensity columns (assuming the first column is wavelength)\n",
    "    intensities_2 = df.iloc[:, 1:]  \n",
    "\n",
    "    # Initialize an empty DataFrame to store normalized intensities\n",
    "    normalized_intensities_2 = pd.DataFrame()\n",
    "\n",
    "    # Iterate over each column (each spectrum)\n",
    "    for column_2 in intensities_2.columns:\n",
    "        spectrum_2 = intensities_2[column_2]\n",
    "        \n",
    "        # Find the maximum intensity value in the spectrum\n",
    "        max_intensity = spectrum_2.max()\n",
    "        \n",
    "        # Normalize the spectrum by dividing each intensity value by the maximum intensity\n",
    "        normalized_spectrum_2 = spectrum_2 / max_intensity\n",
    "        \n",
    "        # Assign the normalized spectrum to the DataFrame of normalized intensities\n",
    "        normalized_intensities_2[column_2] = normalized_spectrum_2\n",
    "\n",
    "    # Combine the wavelength column with normalized intensities\n",
    "    normalized_df_2 = pd.concat([df.iloc[:, 0], normalized_intensities_2], axis=1)\n",
    "\n",
    "    return normalized_df_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d286b96-1e69-44d8-8b3f-05a9e322d617",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE1_Normalized_df = standard_normal_variate_normalization(SAMPLE1_BaselineCorrected_df)\n",
    "SAMPLE2_Normalized_df = standard_normal_variate_normalization(SAMPLE2_BaselineCorrected_df)\n",
    "SAMPLE3_Normalized_df = standard_normal_variate_normalization(SAMPLE3_BaselineCorrected_df)\n",
    "SAMPLE4_Normalized_df = standard_normal_variate_normalization(SAMPLE4_BaselineCorrected_df)\n",
    "SAMPLE5_Normalized_df = standard_normal_variate_normalization(SAMPLE5_BaselineCorrected_df)\n",
    "SAMPLE6_Normalized_df = standard_normal_variate_normalization(SAMPLE6_BaselineCorrected_df)\n",
    "SAMPLE7_Normalized_df = standard_normal_variate_normalization(SAMPLE7_BaselineCorrected_df)\n",
    "SAMPLE8_Normalized_df = standard_normal_variate_normalization(SAMPLE8_BaselineCorrected_df)\n",
    "SAMPLE9_Normalized_df = standard_normal_variate_normalization(SAMPLE9_BaselineCorrected_df)\n",
    "SAMPLE10_Normalized_df = standard_normal_variate_normalization(SAMPLE10_BaselineCorrected_df)\n",
    "SAMPLE11_Normalized_df = standard_normal_variate_normalization(SAMPLE11_BaselineCorrected_df)\n",
    "SAMPLE12_Normalized_df = standard_normal_variate_normalization(SAMPLE12_BaselineCorrected_df)\n",
    "SAMPLE13_Normalized_df = standard_normal_variate_normalization(SAMPLE13_BaselineCorrected_df)\n",
    "SAMPLE14_Normalized_df = standard_normal_variate_normalization(SAMPLE14_BaselineCorrected_df)\n",
    "SAMPLE15_Normalized_df = standard_normal_variate_normalization(SAMPLE15_BaselineCorrected_df)\n",
    "SAMPLE16_Normalized_df = standard_normal_variate_normalization(SAMPLE16_BaselineCorrected_df)\n",
    "SAMPLE17_Normalized_df = standard_normal_variate_normalization(SAMPLE17_BaselineCorrected_df)\n",
    "\n",
    "mean_intensity = SAMPLE1_Normalized_df['sample1_1'].mean()\n",
    "std_intensity = SAMPLE1_Normalized_df['sample1_1'].std()\n",
    "\n",
    "# print(mean_intensity)\n",
    "# print(std_intensity)\n",
    "\n",
    "SAMPLE17_Normalized_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf3fe22-7f2d-4eb6-92c3-ea98ab26f696",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming you have 17 DataFrames stored in a list called `dfs`\n",
    "# Replace this with your actual list of DataFrames\n",
    "all_normalized_df = [SAMPLE1_Normalized_df, SAMPLE2_Normalized_df, SAMPLE3_Normalized_df, SAMPLE4_Normalized_df, SAMPLE5_Normalized_df, SAMPLE6_Normalized_df, SAMPLE7_Normalized_df, SAMPLE8_Normalized_df, SAMPLE9_Normalized_df, SAMPLE10_Normalized_df, SAMPLE11_Normalized_df, SAMPLE12_Normalized_df, SAMPLE13_Normalized_df, SAMPLE14_Normalized_df, SAMPLE15_Normalized_df, SAMPLE16_Normalized_df, SAMPLE17_Normalized_df]\n",
    "\n",
    "# Extract the wavelength column from the first DataFrame\n",
    "wavelength_df = all_normalized_df[0][['wavelength']]\n",
    "# print(wavelength_df)\n",
    "\n",
    "# Concatenate all DataFrames (excluding the 'wavelength' column)\n",
    "combined_df = pd.concat([df.drop(columns='wavelength') for df in all_normalized_df], axis=1)\n",
    "\n",
    "\n",
    "# Add the wavelength column back\n",
    "combined_df = pd.concat([wavelength_df, combined_df], axis=1)\n",
    "\n",
    "combined_df = combined_df.dropna()\n",
    "\n",
    "preprocessed_df = combined_df\n",
    "\n",
    "\n",
    "# Display the combined DataFrame\n",
    "print(combined_df)\n",
    "\n",
    "# Optionally, save the combined DataFrame to a CSV file\n",
    "# combined_df.to_csv('combined_samples.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c967b4-b009-48a6-8d67-9857cab410d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Average the Resultant Spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313409df-ca3b-427f-9228-aee74e650337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def average_intensity(df, intensity_name):\n",
    "#     # Select intensity columns (from column 2 to column 13)\n",
    "#     intensity_columns = df.columns[1:]\n",
    "\n",
    "#     # Calculate the mean of intensity columns\n",
    "#     averaged_intensity = df[intensity_columns].mean(axis=1)\n",
    "\n",
    "#     # Create a new DataFrame with wavelength and averaged intensity\n",
    "#     averaged_df = pd.DataFrame({'wavelength': df['wavelength'], intensity_name: averaged_intensity})\n",
    "\n",
    "#     return averaged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73015028-e982-40dd-bbe1-198dcf4b969a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAMPLE1_Averaged_df = average_intensity(df = SAMPLE1_Normalized_df , intensity_name= 'sample1')\n",
    "# SAMPLE2_Averaged_df = average_intensity(df = SAMPLE2_Normalized_df , intensity_name= 'sample2')\n",
    "# SAMPLE3_Averaged_df = average_intensity(df = SAMPLE3_Normalized_df , intensity_name= 'sample3')\n",
    "# SAMPLE4_Averaged_df = average_intensity(df = SAMPLE4_Normalized_df , intensity_name= 'sample4')\n",
    "# SAMPLE5_Averaged_df = average_intensity(df = SAMPLE5_Normalized_df , intensity_name= 'sample5')\n",
    "# SAMPLE6_Averaged_df = average_intensity(df = SAMPLE6_Normalized_df , intensity_name= 'sample6')\n",
    "# SAMPLE7_Averaged_df = average_intensity(df = SAMPLE7_Normalized_df , intensity_name= 'sample7')\n",
    "# SAMPLE8_Averaged_df = average_intensity(df = SAMPLE8_Normalized_df , intensity_name= 'sample8')\n",
    "# SAMPLE9_Averaged_df = average_intensity(df = SAMPLE9_Normalized_df , intensity_name= 'sample9')\n",
    "# SAMPLE10_Averaged_df = average_intensity(df = SAMPLE10_Normalized_df , intensity_name= 'sample10')\n",
    "# SAMPLE11_Averaged_df = average_intensity(df = SAMPLE11_Normalized_df , intensity_name= 'sample11')\n",
    "# SAMPLE12_Averaged_df = average_intensity(df = SAMPLE12_Normalized_df , intensity_name= 'sample12')\n",
    "# SAMPLE13_Averaged_df = average_intensity(df = SAMPLE13_Normalized_df , intensity_name= 'sample13')\n",
    "# SAMPLE14_Averaged_df = average_intensity(df = SAMPLE14_Normalized_df , intensity_name= 'sample14')\n",
    "# SAMPLE15_Averaged_df = average_intensity(df = SAMPLE15_Normalized_df , intensity_name= 'sample15')\n",
    "# SAMPLE16_Averaged_df = average_intensity(df = SAMPLE16_Normalized_df , intensity_name= 'sample16')\n",
    "# SAMPLE17_Averaged_df = average_intensity(df = SAMPLE17_Normalized_df , intensity_name= 'sample17')\n",
    "\n",
    "# SAMPLE1_Averaged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd21d172-139e-42f5-8489-2f3f6149af73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # List of DataFrame names\n",
    "# data_frames = [SAMPLE1_Averaged_df, SAMPLE2_Averaged_df, SAMPLE3_Averaged_df, \n",
    "#                SAMPLE4_Averaged_df, SAMPLE5_Averaged_df, SAMPLE6_Averaged_df, \n",
    "#                SAMPLE7_Averaged_df, SAMPLE8_Averaged_df ,SAMPLE9_Averaged_df,\n",
    "#                SAMPLE10_Averaged_df,SAMPLE11_Averaged_df,SAMPLE12_Averaged_df,\n",
    "#                SAMPLE13_Averaged_df,SAMPLE14_Averaged_df,SAMPLE15_Averaged_df,\n",
    "#                SAMPLE16_Averaged_df,SAMPLE17_Averaged_df]\n",
    "\n",
    "# # Initialize the new DataFrame with the 'wavelength' column from the first DataFrame\n",
    "# preprocessed_df = data_frames[0][['wavelength']].copy()\n",
    "\n",
    "# # Add 'sample' columns from each DataFrame\n",
    "# for i, df in enumerate(data_frames):\n",
    "#     sample_column = f'sample{i+1}'\n",
    "#     preprocessed_df[sample_column] = df[sample_column]\n",
    "\n",
    "# # Display the new DataFrame\n",
    "# preprocessed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547b9ca0-3af3-4c33-a5f5-7b13b7761337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessed_df_Plot = figure(title = 'Preprocessed Data Plot' , x_axis_label = 'Wavelength' , y_axis_label = 'Intensity')\n",
    "\n",
    "# Preprocessed_df_Plot.line(preprocessed_df.wavelength,preprocessed_df.sample1 , line_width = 2, color =\"blue\" )\n",
    "# #####################################################################################################################\n",
    "# Preprocessed_df_Plot.line(preprocessed_df.wavelength,preprocessed_df.sample2 , line_width = 2, color =\"orange\" )\n",
    "# ###############################################################################################################\n",
    "# Preprocessed_df_Plot.line(preprocessed_df.wavelength,preprocessed_df.sample3 , line_width = 2, color =\"green\")\n",
    "# ######################################################################################################################\n",
    "# Preprocessed_df_Plot.line(preprocessed_df.wavelength,preprocessed_df.sample4 , line_width = 2, color =\"red\")\n",
    "# ##############################################################################################################\n",
    "# Preprocessed_df_Plot.line(preprocessed_df.wavelength,preprocessed_df.sample5 , line_width = 2, color =\"purple\")\n",
    "# ###############################################################################################################\n",
    "# Preprocessed_df_Plot.line(preprocessed_df.wavelength,preprocessed_df.sample6 , line_width = 2, color =\"brown\")\n",
    "# ####################################################################################################################\n",
    "# Preprocessed_df_Plot.line(preprocessed_df.wavelength,preprocessed_df.sample7 , line_width = 2, color =\"pink\")\n",
    "# ##################################################################################################################\n",
    "# Preprocessed_df_Plot.line(preprocessed_df.wavelength,preprocessed_df.sample8 , line_width = 2, color =\"gray\")\n",
    "# ###################################################################################################################\n",
    "# Preprocessed_df_Plot.line(preprocessed_df.wavelength,preprocessed_df.sample9 , line_width = 2, color =\"olive\")\n",
    "# ###################################################################################################################\n",
    "# Preprocessed_df_Plot.line(preprocessed_df.wavelength,preprocessed_df.sample10 , line_width = 2, color =\"Cyan\")\n",
    "# ###################################################################################################################\n",
    "# Preprocessed_df_Plot.line(preprocessed_df.wavelength,preprocessed_df.sample11 , line_width = 2, color =\"black\")\n",
    "# ###################################################################################################################\n",
    "# Preprocessed_df_Plot.line(preprocessed_df.wavelength,preprocessed_df.sample12 , line_width = 2, color =\"tomato\")\n",
    "# ###################################################################################################################\n",
    "# Preprocessed_df_Plot.line(preprocessed_df.wavelength,preprocessed_df.sample13 , line_width = 2, color =\"steelblue\")\n",
    "# ###################################################################################################################\n",
    "# Preprocessed_df_Plot.line(preprocessed_df.wavelength,preprocessed_df.sample14 , line_width = 2, color =\"limegreen\")\n",
    "# ###################################################################################################################\n",
    "# Preprocessed_df_Plot.line(preprocessed_df.wavelength,preprocessed_df.sample15 , line_width = 2, color =\"deeppink\")\n",
    "# ###################################################################################################################\n",
    "# Preprocessed_df_Plot.line(preprocessed_df.wavelength,preprocessed_df.sample16 , line_width = 2, color =\"gold\")\n",
    "# ###################################################################################################################\n",
    "# Preprocessed_df_Plot.line(preprocessed_df.wavelength,preprocessed_df.sample17 , line_width = 2, color =\"indigo\")\n",
    "# ###################################################################################################################\n",
    "\n",
    "# Preprocessed_df_Plot.width = 900\n",
    "# Preprocessed_df_Plot.height = 500\n",
    "# show(Preprocessed_df_Plot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5432ee-737e-4d14-a9d8-98ba4eee7f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "preprocessed_df_trasnposed = preprocessed_df.T\n",
    "print(preprocessed_df_trasnposed)\n",
    "\n",
    "########################################\n",
    "\n",
    "labels = preprocessed_df_trasnposed[1:].index.tolist()  #collecting the labels from the samples\n",
    "print(labels)\n",
    "\n",
    "############################################\n",
    "\n",
    "selected_samples = preprocessed_df_trasnposed.loc['sample9_9':'sample17_10']\n",
    "X = selected_samples.values\n",
    "\n",
    "\n",
    "selected_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2fc3ce-7382-4ca9-a05c-275d94b66478",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determining the Number of Principle Component Required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45a717e-62df-429f-8c02-0300db82ac82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt1\n",
    "\n",
    "\n",
    "\n",
    "pca = PCA(n_components=17)\n",
    "\n",
    "# Perform PCA\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Explained variance ratio\n",
    "explained_variance_ratio = pca.explained_variance_ratio_  # Access from the PCA object\n",
    "\n",
    "percentage_list = []\n",
    "for ite in explained_variance_ratio:\n",
    "    percentage = ite * 100\n",
    "    percentage_list.append(percentage)\n",
    "\n",
    "print(percentage_list)\n",
    "\n",
    "\n",
    "# PCA components\n",
    "pca_components = pca.components_\n",
    "\n",
    "# Plot the explained variance ratio\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio, 'bo-', linewidth=2)\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.title('Explained Variance Ratio by Principal Component')\n",
    "plt.savefig('Explained_Variance_batch3.png', bbox_inches='tight', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Create a bar plot\n",
    "plt1.figure(figsize=(10, 6))\n",
    "plt1.bar(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio, color='skyblue')\n",
    "plt1.xlabel('Principal Component')\n",
    "plt1.ylabel('Explained Variance Ratio')\n",
    "plt1.title('Explained Variance Ratio by Principal Component')\n",
    "plt1.xticks(range(1, len(explained_variance_ratio) + 1))  # Ensure x-ticks are labeled correctly\n",
    "plt.savefig('Explained_Variance_bargraph_batch3.png', bbox_inches='tight', dpi=300)\n",
    "plt1.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9048af-2498-4057-b51f-6cf4b75c904b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.decomposition import PCA\n",
    "# import matplotlib.pyplot as plt2\n",
    "\n",
    "# # Assuming preprocessed_df_trasnposed and X are already defined as per your provided code\n",
    "\n",
    "# # Initialize PCA with 2 components for 2D plot\n",
    "# pca_2d = PCA(n_components=4)\n",
    "\n",
    "# # Perform PCA to reduce to 2 dimensions\n",
    "# X_pca_2d = pca_2d.fit_transform(X)\n",
    "\n",
    "# # Create a DataFrame for easy plotting\n",
    "# df_pca_2d = pd.DataFrame(X_pca_2d, columns=['PC1', 'PC2' , 'PC3' , 'PC4'], index=selected_samples.index)\n",
    "\n",
    "# # Plot the 2D PCA results\n",
    "# plt.figure(figsize=(10, 8))\n",
    "\n",
    "# # Plot each sample with its respective label\n",
    "# plt.scatter(df_pca_2d['PC1'], df_pca_2d['PC3'], c='blue', marker='x')\n",
    "\n",
    "# # Adding labels to the plot\n",
    "# for i, label in enumerate(df_pca_2d.index):\n",
    "#     plt.text(df_pca_2d['PC1'][i], df_pca_2d['PC3'][i], label, fontsize=5)\n",
    "\n",
    "# plt.xlabel('Principal Component 1')\n",
    "# plt.ylabel('Principal Component 2')\n",
    "# plt.title('2D PCA of the Data')\n",
    "# plt.grid(True)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4148ad7-69f8-4355-b164-eed5b2364c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Initialize PCA with 2 components\n",
    "pca_2d = PCA(n_components=3)\n",
    "\n",
    "# Perform PCA to reduce to 2 dimensions\n",
    "X_pca_2d = pca_2d.fit_transform(X)\n",
    "\n",
    "# Create a DataFrame for easy plotting\n",
    "df_pca_2d = pd.DataFrame(X_pca_2d, columns=['PC1', 'PC2' , 'PC3'], index=selected_samples.index)\n",
    "\n",
    "# Define markers for different sample groups\n",
    "markers = {\n",
    "    'sample1': 'o',\n",
    "    'sample2': '^',\n",
    "    'sample3': 's',\n",
    "    'sample4': 'p',\n",
    "    'sample5': '*',\n",
    "    'sample6': 'D',\n",
    "    'sample7': 'H',\n",
    "    'sample8': '+',\n",
    "    'sample9': 'x',\n",
    "    'sample10': '|',\n",
    "    'sample11': '_',\n",
    "    'sample12': '1',\n",
    "    'sample13': '2',\n",
    "    'sample14': '3',\n",
    "    'sample15': '4',\n",
    "    'sample16': 'v',\n",
    "    'sample17': '<'\n",
    "}\n",
    "\n",
    "# Extract the base name of each sample to determine its group\n",
    "sample_groups = {}\n",
    "for sample in df_pca_2d.index:\n",
    "    base_name = sample.split('_')[0]  # Get the base name like 'sample1', 'sample2', etc.\n",
    "    if base_name in markers:\n",
    "        if base_name not in sample_groups:\n",
    "            sample_groups[base_name] = []\n",
    "        sample_groups[base_name].append(sample)\n",
    "\n",
    "# Plot the 2D PCA results\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "# Plot each group with a different marker\n",
    "for group, samples in sample_groups.items():\n",
    "    marker = markers[group]  # Use the assigned marker for this group\n",
    "    group_df = df_pca_2d.loc[samples]\n",
    "    ax.scatter(group_df['PC2'], group_df['PC3'], \n",
    "               c=np.random.rand(3,),  # Random color for each group\n",
    "               marker=marker, s=50, edgecolor='k', label=group)\n",
    "\n",
    "# Add a legend to show which marker corresponds to which sample group\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(handles, labels, bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "\n",
    "# Set labels and title with better font and style\n",
    "ax.set_xlabel('Principal Component 2', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('Principal Component 3', fontsize=14, fontweight='bold')\n",
    "ax.set_title('2D PCA of the Data', fontsize=16, fontweight='bold')\n",
    "\n",
    "\n",
    "# Customize the grid and background\n",
    "ax.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "plt.savefig('pca_batch3_pc2_pc3.png', bbox_inches='tight', dpi=300)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f8fdd5-2a04-4e60-9315-cdc7ba32bde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import numpy as np\n",
    "\n",
    "# Initialize PCA with 3 components\n",
    "pca_3d = PCA(n_components=3)\n",
    "\n",
    "# Perform PCA to reduce to 3 dimensions\n",
    "X_pca_3d = pca_3d.fit_transform(X)\n",
    "\n",
    "# Create a DataFrame for easy plotting\n",
    "df_pca_3d = pd.DataFrame(X_pca_3d, columns=['PC1', 'PC2', 'PC3'], index=selected_samples.index)\n",
    "\n",
    "# Define markers for different sample groups\n",
    "# Here we're using sample categories like 'sample1', 'sample2', etc.\n",
    "# You might need to adjust this according to your actual sample groupings.\n",
    "markers = {\n",
    "    'sample1': 'o',\n",
    "    'sample2': '^',\n",
    "    'sample3': 's',\n",
    "    'sample4': 'p',\n",
    "    'sample5': '*',\n",
    "    'sample6': 'D',\n",
    "    'sample7': 'H',\n",
    "    'sample8': '+',\n",
    "    'sample9': 'x',\n",
    "    'sample10': '|',\n",
    "    'sample11': '_',\n",
    "    'sample12': '1',\n",
    "    'sample13': '2',\n",
    "    'sample14': '3',\n",
    "    'sample15': '4',\n",
    "    'sample16': 'v',\n",
    "    'sample17': '<'\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Plot the 3D PCA results\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Extract the base name of each sample to determine its group\n",
    "sample_groups = {}\n",
    "for sample in df_pca_3d.index:\n",
    "    base_name = sample.split('_')[0]  # Get the base name like 'sample1', 'sample2', etc.\n",
    "    if base_name in markers:\n",
    "        if base_name not in sample_groups:\n",
    "            sample_groups[base_name] = []\n",
    "        sample_groups[base_name].append(sample)\n",
    "\n",
    "print(sample_groups)\n",
    "\n",
    "# Plot each group with a different marker\n",
    "for group, samples in sample_groups.items():\n",
    "    marker = markers[group]  # Use the assigned marker for this group\n",
    "    group_df = df_pca_3d.loc[samples]\n",
    "    ax.scatter(group_df['PC1'], group_df['PC2'], group_df['PC3'], \n",
    "               c=np.random.rand(3,),  # Random color for each group\n",
    "               marker=marker, s=50, edgecolor='k', label=group)\n",
    "\n",
    "# Add a legend to show which marker corresponds to which sample group\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(handles, labels, bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "# Set labels and title with better font and style\n",
    "ax.set_xlabel('Principal Component 1', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('Principal Component 2', fontsize=14, fontweight='bold')\n",
    "ax.set_zlabel('Principal Component 3', fontsize=14, fontweight='bold')\n",
    "ax.set_title('3D PCA of the Data', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Customize the grid and background\n",
    "ax.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "plt.savefig('pca_batch3_pc1_pc2_pc3.png', bbox_inches='tight', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e102a0-b737-4477-83ee-e3848aa0acd6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
