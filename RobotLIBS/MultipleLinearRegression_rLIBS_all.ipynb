{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4392714e-2e7c-4828-91aa-b8f81294591f",
   "metadata": {},
   "source": [
    "# IMPORTING THE LIBRARIES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d525889-7089-450b-8158-612b0ddafbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb12269-8ea9-48b9-bb59-b6a6fe3825b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.pyplot as plt1 \n",
    "\n",
    "import statistics\n",
    "import os\n",
    "\n",
    "###############################################\n",
    "from peakutils import indexes\n",
    "from peakutils import baseline\n",
    "from scipy.signal import find_peaks as fp\n",
    "from scipy.signal import savgol_filter \n",
    "\n",
    "###############################################\n",
    "from bokeh.plotting import figure , show\n",
    "from pybaselines import whittaker as pl\n",
    "\n",
    "\n",
    "\n",
    "from bokeh.io import output_notebook\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdb69c6-3ad7-4ef0-9464-62b03f62397c",
   "metadata": {},
   "source": [
    "# DATA ARRANGEMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b58fa81-9ce3-4234-8b5a-b858bddc483d",
   "metadata": {},
   "source": [
    "In the main directory we can see that there are 8 subfolders. \n",
    "\n",
    "Each subfolder have almost 12 spectras per sample , the idea behind that would be , instead of having just one spectra per sample , and to just rely on one information , its always better have to multiple measurements per samples , and then this could be used for building the Calibration Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b23c0c-b3ac-4b59-924f-ef0d38a007a7",
   "metadata": {},
   "source": [
    "Instead of having 12 different csv per samples , its always good to have a single dataframe -> This new dataframe will have 1st column as wavelength , and 2nd -13th column as Intensities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0ecf88-6675-4703-b9b8-9f0d6835f6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(folder_path):\n",
    "    # List to store DataFrames for intensity columns\n",
    "    intensity_dfs = []\n",
    "\n",
    "    # List to store file names\n",
    "    file_names = []\n",
    "\n",
    "    # Get a list of .txt files in the folder\n",
    "    txt_files = [file_name for file_name in os.listdir(folder_path) if file_name.endswith('.txt')]\n",
    "\n",
    "    # Sort the .txt files based on their numerical order\n",
    "    # txt_files.sort(key=lambda x: int(re.search(r'_(\\d+)\\.txt', x).group(1)))\n",
    "\n",
    "    # Read the wavelength values from the first file\n",
    "    first_file_path = os.path.join(folder_path, txt_files[0])\n",
    "    wavelength_df = pd.read_csv(first_file_path, header=None, delimiter=';', usecols=[0], names=['wavelength'])\n",
    "\n",
    "    # Loop through each file in ascending order\n",
    "    for file_name in txt_files:\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        \n",
    "        # Read intensity values from each file into a DataFrame\n",
    "        intensity_df = pd.read_csv(file_path, header=None, delimiter=';', usecols=[1], names=['intensity'])\n",
    "        \n",
    "        # Store intensity DataFrame\n",
    "        intensity_dfs.append(intensity_df)\n",
    "        \n",
    "        # Store file name\n",
    "        file_names.append(os.path.splitext(file_name)[0])\n",
    "\n",
    "    # Concatenate intensity DataFrames\n",
    "    result_df = pd.concat(intensity_dfs, axis=1)\n",
    "\n",
    "    # Add the wavelength column to the result DataFrame\n",
    "    result_df = pd.concat([wavelength_df, result_df], axis=1)\n",
    "\n",
    "    # Rename the columns with file names\n",
    "    result_df.columns = ['wavelength'] + file_names\n",
    "\n",
    "    return result_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7b2157-acda-46f4-9d9a-226024715bc2",
   "metadata": {},
   "source": [
    "Loading the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a46d114-05e7-46d0-93ef-59e912c20da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE1_Raw_df = load_data('Batch_2/sample_1')\n",
    "###################################################\n",
    "SAMPLE2_Raw_df = load_data('Batch_2/sample_2')\n",
    "###################################################\n",
    "SAMPLE3_Raw_df = load_data('Batch_2/sample_3')\n",
    "###################################################\n",
    "SAMPLE4_Raw_df = load_data('Batch_2/sample_4')\n",
    "###################################################\n",
    "SAMPLE5_Raw_df = load_data('Batch_2/sample_5')\n",
    "###################################################\n",
    "SAMPLE6_Raw_df = load_data('Batch_2/sample_6')\n",
    "###################################################\n",
    "SAMPLE7_Raw_df = load_data('Batch_2/sample_7')\n",
    "####################################################\n",
    "SAMPLE8_Raw_df = load_data('Batch_2/sample_8')\n",
    "####################################################\n",
    "SAMPLE9_Raw_df = load_data('Batch_3/sample_9')\n",
    "####################################################\n",
    "SAMPLE10_Raw_df = load_data('Batch_3/sample_10')\n",
    "####################################################\n",
    "SAMPLE11_Raw_df = load_data('Batch_3/sample_11')\n",
    "####################################################\n",
    "SAMPLE12_Raw_df = load_data('Batch_3/sample_12')\n",
    "####################################################\n",
    "SAMPLE13_Raw_df = load_data('Batch_3/sample_13')\n",
    "####################################################\n",
    "SAMPLE14_Raw_df = load_data('Batch_3/sample_14')\n",
    "####################################################\n",
    "SAMPLE15_Raw_df = load_data('Batch_3/sample_15')\n",
    "####################################################\n",
    "SAMPLE16_Raw_df = load_data('Batch_3/sample_16')\n",
    "####################################################\n",
    "SAMPLE17_Raw_df = load_data('Batch_3/sample_17')\n",
    "####################################################\n",
    "\n",
    "SAMPLE1_Raw_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a420d66a-2223-404e-be46-96a4d4d93c40",
   "metadata": {},
   "source": [
    "# Peak Selection and Data Trimming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90a6244-3918-411b-ab77-652212e87e72",
   "metadata": {},
   "source": [
    "The dataframe  which we have is very big ,it could be trimmed now according to the wavelength , by adjusting two parameters \"Wavelength_Min\" , \"Wavelength_Max\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fbcf80-80b5-4f24-b0d3-2dc01cfbed35",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Getting the full wavelength\n",
    "Wavelength_Min = 230\n",
    "Wavelength_Max = 540\n",
    "\n",
    "\n",
    "SAMPLE1_Select_df = SAMPLE1_Raw_df[(SAMPLE1_Raw_df['wavelength'] >= Wavelength_Min) & (SAMPLE1_Raw_df['wavelength'] <= Wavelength_Max)]\n",
    "SAMPLE2_Select_df = SAMPLE2_Raw_df[(SAMPLE2_Raw_df['wavelength'] >= Wavelength_Min) & (SAMPLE2_Raw_df['wavelength'] <= Wavelength_Max)]\n",
    "SAMPLE3_Select_df = SAMPLE3_Raw_df[(SAMPLE3_Raw_df['wavelength'] >= Wavelength_Min) & (SAMPLE3_Raw_df['wavelength'] <= Wavelength_Max)]\n",
    "SAMPLE4_Select_df = SAMPLE4_Raw_df[(SAMPLE4_Raw_df['wavelength'] >= Wavelength_Min) & (SAMPLE4_Raw_df['wavelength'] <= Wavelength_Max)]\n",
    "SAMPLE5_Select_df = SAMPLE5_Raw_df[(SAMPLE5_Raw_df['wavelength'] >= Wavelength_Min) & (SAMPLE5_Raw_df['wavelength'] <= Wavelength_Max)]\n",
    "SAMPLE6_Select_df = SAMPLE6_Raw_df[(SAMPLE6_Raw_df['wavelength'] >= Wavelength_Min) & (SAMPLE6_Raw_df['wavelength'] <= Wavelength_Max)]\n",
    "SAMPLE7_Select_df = SAMPLE7_Raw_df[(SAMPLE7_Raw_df['wavelength'] >= Wavelength_Min) & (SAMPLE7_Raw_df['wavelength'] <= Wavelength_Max)]\n",
    "SAMPLE8_Select_df = SAMPLE8_Raw_df[(SAMPLE8_Raw_df['wavelength'] >= Wavelength_Min) & (SAMPLE8_Raw_df['wavelength'] <= Wavelength_Max)]\n",
    "SAMPLE9_Select_df = SAMPLE9_Raw_df[(SAMPLE9_Raw_df['wavelength'] >= Wavelength_Min) & (SAMPLE9_Raw_df['wavelength'] <= Wavelength_Max)]\n",
    "SAMPLE10_Select_df = SAMPLE10_Raw_df[(SAMPLE10_Raw_df['wavelength'] >= Wavelength_Min) & (SAMPLE10_Raw_df['wavelength'] <= Wavelength_Max)]\n",
    "SAMPLE11_Select_df = SAMPLE11_Raw_df[(SAMPLE11_Raw_df['wavelength'] >= Wavelength_Min) & (SAMPLE11_Raw_df['wavelength'] <= Wavelength_Max)]\n",
    "SAMPLE12_Select_df = SAMPLE12_Raw_df[(SAMPLE12_Raw_df['wavelength'] >= Wavelength_Min) & (SAMPLE12_Raw_df['wavelength'] <= Wavelength_Max)]\n",
    "SAMPLE13_Select_df = SAMPLE13_Raw_df[(SAMPLE13_Raw_df['wavelength'] >= Wavelength_Min) & (SAMPLE13_Raw_df['wavelength'] <= Wavelength_Max)]\n",
    "SAMPLE14_Select_df = SAMPLE14_Raw_df[(SAMPLE14_Raw_df['wavelength'] >= Wavelength_Min) & (SAMPLE14_Raw_df['wavelength'] <= Wavelength_Max)]\n",
    "SAMPLE15_Select_df = SAMPLE15_Raw_df[(SAMPLE15_Raw_df['wavelength'] >= Wavelength_Min) & (SAMPLE15_Raw_df['wavelength'] <= Wavelength_Max)]\n",
    "SAMPLE16_Select_df = SAMPLE16_Raw_df[(SAMPLE16_Raw_df['wavelength'] >= Wavelength_Min) & (SAMPLE16_Raw_df['wavelength'] <= Wavelength_Max)]\n",
    "SAMPLE17_Select_df = SAMPLE17_Raw_df[(SAMPLE17_Raw_df['wavelength'] >= Wavelength_Min) & (SAMPLE17_Raw_df['wavelength'] <= Wavelength_Max)]\n",
    "\n",
    "\n",
    "SAMPLE1_Select_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0c9fff-fb15-4539-9330-9ed39eecf07a",
   "metadata": {},
   "source": [
    "Lets plot a line plot ,to get a better picture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2063c4-4f62-4727-aae2-333fb4e7d4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Selected_df_Plot = figure(title = 'Selected Data Plot' , x_axis_label = 'Wavelength' , y_axis_label = 'Intensity')\n",
    "\n",
    "Selected_df_Plot.line(SAMPLE1_Select_df.wavelength,SAMPLE1_Select_df.sample1_6 , line_width = 2, color =\"blue\" )\n",
    "#####################################################################################################################\n",
    "Selected_df_Plot.line(SAMPLE2_Select_df.wavelength,SAMPLE2_Select_df.sample2_6 , line_width = 2, color =\"orange\" )\n",
    "###############################################################################################################\n",
    "Selected_df_Plot.line(SAMPLE3_Select_df.wavelength,SAMPLE3_Select_df.sample3_6 , line_width = 2, color =\"green\")\n",
    "######################################################################################################################\n",
    "Selected_df_Plot.line(SAMPLE4_Select_df.wavelength,SAMPLE4_Select_df.sample4_6 , line_width = 2, color =\"red\")\n",
    "##############################################################################################################\n",
    "Selected_df_Plot.line(SAMPLE5_Select_df.wavelength,SAMPLE5_Select_df.sample5_6 , line_width = 2, color =\"purple\")\n",
    "###############################################################################################################\n",
    "Selected_df_Plot.line(SAMPLE6_Select_df.wavelength,SAMPLE6_Select_df.sample6_6 , line_width = 2, color =\"brown\")\n",
    "####################################################################################################################\n",
    "Selected_df_Plot.line(SAMPLE7_Select_df.wavelength,SAMPLE7_Select_df.sample7_6 , line_width = 2, color =\"pink\")\n",
    "##################################################################################################################\n",
    "Selected_df_Plot.line(SAMPLE8_Select_df.wavelength,SAMPLE8_Select_df.sample8_6 , line_width = 2, color =\"gray\")\n",
    "###################################################################################################################\n",
    "Selected_df_Plot.line(SAMPLE9_Select_df.wavelength,SAMPLE9_Select_df.sample9_6 , line_width = 2, color =\"olive\")\n",
    "###################################################################################################################\n",
    "Selected_df_Plot.line(SAMPLE10_Select_df.wavelength,SAMPLE10_Select_df.sample10_6 , line_width = 2, color =\"Cyan\")\n",
    "###################################################################################################################\n",
    "Selected_df_Plot.line(SAMPLE11_Select_df.wavelength,SAMPLE11_Select_df.sample11_6 , line_width = 2, color =\"black\")\n",
    "###################################################################################################################\n",
    "Selected_df_Plot.line(SAMPLE12_Select_df.wavelength,SAMPLE12_Select_df.sample12_6 , line_width = 2, color =\"tomato\")\n",
    "###################################################################################################################\n",
    "Selected_df_Plot.line(SAMPLE13_Select_df.wavelength,SAMPLE13_Select_df.sample13_6 , line_width = 2, color =\"steelblue\")\n",
    "###################################################################################################################\n",
    "Selected_df_Plot.line(SAMPLE14_Select_df.wavelength,SAMPLE14_Select_df.sample14_6 , line_width = 2, color =\"limegreen\")\n",
    "###################################################################################################################\n",
    "Selected_df_Plot.line(SAMPLE15_Select_df.wavelength,SAMPLE15_Select_df.sample15_6 , line_width = 2, color =\"deeppink\")\n",
    "###################################################################################################################\n",
    "Selected_df_Plot.line(SAMPLE16_Select_df.wavelength,SAMPLE16_Select_df.sample16_6 , line_width = 2, color =\"gold\")\n",
    "###################################################################################################################\n",
    "Selected_df_Plot.line(SAMPLE17_Select_df.wavelength,SAMPLE17_Select_df.sample17_6 , line_width = 2, color =\"indigo\")\n",
    "###################################################################################################################\n",
    "\n",
    "Selected_df_Plot.width = 1200\n",
    "Selected_df_Plot.height = 500\n",
    "show(Selected_df_Plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32553d02-101b-463e-91a9-0bec5f82ba88",
   "metadata": {},
   "source": [
    "# Data Preprocessing of the Spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105566ee-f241-48a2-8c71-10d3c13bc5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_correction(df):\n",
    "    \"\"\"\n",
    "    Perform baseline correction on the intensity columns of the input DataFrame and create a new DataFrame with corrected values.\n",
    "    \n",
    "    Parameters:\n",
    "        df (DataFrame): Input DataFrame containing the wavelength and intensity columns.\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: New DataFrame with baseline-corrected intensity columns and the same wavelength column as the input DataFrame.\n",
    "    \"\"\"\n",
    "    # Copy the 'wavelength' column from the input DataFrame\n",
    "    new_df = pd.DataFrame({'wavelength': df['wavelength']})\n",
    "    \n",
    "    # Perform baseline correction for each intensity column and add them to the new DataFrame\n",
    "    for col in df.columns[1:]:  # Exclude the 'wavelength' column\n",
    "        baseline, _ = pl.asls(df[col])\n",
    "        corrected_values = df[col] - baseline\n",
    "        new_df[col] = corrected_values\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac0636c-323a-47ae-9af1-1f1c6443ac04",
   "metadata": {},
   "source": [
    "The above plot eventhough a spectra , is still a Raw Spectra , which still has lot of Artifects , before proceeding for the Univariate Calibration , its important to Pre Process the Raw Spectra accordingly. Various Pre Processing Techniques could be used here :- \n",
    "\n",
    "1) Baseline Correction - Very Very little  background radiation is still present in the spectra, which corresponds to the spectral baseline and imposes difficulties for quantitative elemental analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdddbd95-1f4e-4c7e-8d6e-6a5cc4f0a404",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE1_BaselineCorrected_df = baseline_correction(SAMPLE1_Select_df)\n",
    "SAMPLE2_BaselineCorrected_df = baseline_correction(SAMPLE2_Select_df)\n",
    "SAMPLE3_BaselineCorrected_df = baseline_correction(SAMPLE3_Select_df)\n",
    "SAMPLE4_BaselineCorrected_df = baseline_correction(SAMPLE4_Select_df)\n",
    "SAMPLE5_BaselineCorrected_df = baseline_correction(SAMPLE5_Select_df)\n",
    "SAMPLE6_BaselineCorrected_df = baseline_correction(SAMPLE6_Select_df)\n",
    "SAMPLE7_BaselineCorrected_df = baseline_correction(SAMPLE7_Select_df)\n",
    "SAMPLE8_BaselineCorrected_df = baseline_correction(SAMPLE8_Select_df)\n",
    "SAMPLE9_BaselineCorrected_df = baseline_correction(SAMPLE9_Select_df)\n",
    "SAMPLE10_BaselineCorrected_df = baseline_correction(SAMPLE10_Select_df)\n",
    "SAMPLE11_BaselineCorrected_df = baseline_correction(SAMPLE11_Select_df)\n",
    "SAMPLE12_BaselineCorrected_df = baseline_correction(SAMPLE12_Select_df)\n",
    "SAMPLE13_BaselineCorrected_df = baseline_correction(SAMPLE13_Select_df)\n",
    "SAMPLE14_BaselineCorrected_df = baseline_correction(SAMPLE14_Select_df)\n",
    "SAMPLE15_BaselineCorrected_df = baseline_correction(SAMPLE15_Select_df)\n",
    "SAMPLE16_BaselineCorrected_df = baseline_correction(SAMPLE16_Select_df)\n",
    "SAMPLE17_BaselineCorrected_df = baseline_correction(SAMPLE17_Select_df)\n",
    "\n",
    "SAMPLE1_BaselineCorrected_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3835fd-106c-49d1-aeb9-6308caa4b4e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Baseline_Correction_Plot = figure(title = 'Baseline Correction' , x_axis_label = 'Wavelength' , y_axis_label = 'Intensity')\n",
    "\n",
    "Baseline_Correction_Plot.line(SAMPLE4_Select_df.wavelength,SAMPLE4_Select_df.sample4_1 , line_width = 2, color =\"red\" )\n",
    "Baseline_Correction_Plot.line(SAMPLE4_BaselineCorrected_df.wavelength,SAMPLE4_BaselineCorrected_df.sample4_1 , line_width = 2, color =\"green\" )\n",
    "\n",
    "\n",
    "Baseline_Correction_Plot.width = 900\n",
    "Baseline_Correction_Plot.height = 500\n",
    "show(Baseline_Correction_Plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c6e970-ceb2-4d8f-8783-76534447c1c2",
   "metadata": {},
   "source": [
    "2) Normalization - Its usually noticed that , for a measurement of a similar sample ,there is a lot of scattering in the intensities , this typical artifect is called scattering.\n",
    "\n",
    "For Instance , you could see the plot above , Though this are the plots from the same sample SAMPLE1 , measured on 12 different Areas , its quite visible , for some  spectras , the peak heights or Intensities are not same.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7458b579-db93-4227-9d22-9408fc61d680",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def standard_normal_variate_normalization(df):\n",
    "#     # Selecting only the intensity columns (assuming the first column is wavelength)\n",
    "#     intensities = df.iloc[:, 1:]\n",
    "\n",
    "#     # Calculating mean and standard deviation for each column\n",
    "#     means = intensities.mean(axis=0)\n",
    "#     stds = intensities.std(axis=0)\n",
    "\n",
    "#     # Applying standard normal variate normalization column-wise\n",
    "#     normalized_intensities = (intensities - means) / stds\n",
    "\n",
    "#     # Combining the wavelength column with normalized intensities\n",
    "#     normalized_df = pd.concat([df.iloc[:, 0], normalized_intensities], axis=1)\n",
    "\n",
    "#     # Calculating mean and standard deviation for each normalized column\n",
    "#     normalized_means = normalized_intensities.mean(axis=0)\n",
    "#     normalized_stds = normalized_intensities.std(axis=0)\n",
    "\n",
    "#     return normalized_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f3937e-3ebd-400c-8b42-dc858abfd4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_normal_variate_normalization(df):\n",
    "    # Select columns containing intensities\n",
    "    intensity_cols = df.columns[1:]  #  columns 2 to 13 are intensities and are stored in intensity_cols\n",
    "\n",
    "    # SVN normalization\n",
    "    for col in intensity_cols:\n",
    "        mean_intensity = df[col].mean()\n",
    "        std_intensity = df[col].std()\n",
    "        df[col] = (df[col] - mean_intensity) / std_intensity\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be861edd-1e45-495d-9b6e-8f62e52d089a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def total_intensity_normalization(df):\n",
    "    # Selecting only the intensity columns (assuming the first column is wavelength)\n",
    "    intensities = df.iloc[:, 1:]  \n",
    "\n",
    "    # Initialize an empty DataFrame to store normalized intensities\n",
    "    normalized_intensities = pd.DataFrame()\n",
    "\n",
    "    # Iterate over each column (each spectrum)\n",
    "    for column in intensities.columns:\n",
    "        spectrum = intensities[column]\n",
    "        total_intensity = spectrum.sum()\n",
    "        \n",
    "        # Normalize the spectrum by dividing each intensity value by the total intensity\n",
    "        normalized_spectrum = spectrum / total_intensity\n",
    "        \n",
    "        # Append the normalized spectrum to the DataFrame of normalized intensities\n",
    "        normalized_intensities[column] = normalized_spectrum\n",
    "\n",
    "    # Combine the wavelength column with normalized intensities\n",
    "    normalized_df = pd.concat([df.iloc[:, 0], normalized_intensities], axis=1)\n",
    "\n",
    "    return normalized_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be72163b-e6ad-415c-a3d3-84d6ae5bc510",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def unit_norm_normalization(df):\n",
    "    # Selecting only the intensity columns (assuming the first column is wavelength)\n",
    "    intensities_1 = df.iloc[:, 1:]  \n",
    "    \n",
    "    # Initialize an empty DataFrame to store normalized intensities\n",
    "    normalized_intensities_1 = pd.DataFrame()\n",
    "\n",
    "    # Iterate over each column (each spectrum)\n",
    "    for column_1 in intensities_1.columns:\n",
    "        spectrum_1 = intensities_1[column_1]\n",
    "        \n",
    "        # Calculate the norm of the spectrum using a simple equation\n",
    "        spectrum_norm = np.sqrt(np.sum(spectrum_1 ** 2))\n",
    "        \n",
    "        # Normalize the spectrum by dividing each intensity value by its norm\n",
    "        normalized_spectrum_1 = spectrum_1 / spectrum_norm\n",
    "        \n",
    "        # Assign the normalized spectrum to the DataFrame of normalized intensities\n",
    "        normalized_intensities_1[column_1] = normalized_spectrum_1\n",
    "\n",
    "    # Combine the wavelength column with normalized intensities\n",
    "    normalized_df_1 = pd.concat([df.iloc[:, 0], normalized_intensities_1], axis=1)\n",
    "    \n",
    "    return normalized_df_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca606ff-3222-4233-a594-04c979996677",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def max_intensity_normalization(df):\n",
    "    # Selecting only the intensity columns (assuming the first column is wavelength)\n",
    "    intensities_2 = df.iloc[:, 1:]  \n",
    "\n",
    "    # Initialize an empty DataFrame to store normalized intensities\n",
    "    normalized_intensities_2 = pd.DataFrame()\n",
    "\n",
    "    # Iterate over each column (each spectrum)\n",
    "    for column_2 in intensities_2.columns:\n",
    "        spectrum_2 = intensities_2[column_2]\n",
    "        \n",
    "        # Find the maximum intensity value in the spectrum\n",
    "        max_intensity = spectrum_2.max()\n",
    "        \n",
    "        # Normalize the spectrum by dividing each intensity value by the maximum intensity\n",
    "        normalized_spectrum_2 = spectrum_2 / max_intensity\n",
    "        \n",
    "        # Assign the normalized spectrum to the DataFrame of normalized intensities\n",
    "        normalized_intensities_2[column_2] = normalized_spectrum_2\n",
    "\n",
    "    # Combine the wavelength column with normalized intensities\n",
    "    normalized_df_2 = pd.concat([df.iloc[:, 0], normalized_intensities_2], axis=1)\n",
    "\n",
    "    return normalized_df_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d286b96-1e69-44d8-8b3f-05a9e322d617",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE1_Normalized_df = total_intensity_normalization(SAMPLE1_BaselineCorrected_df)\n",
    "SAMPLE2_Normalized_df = total_intensity_normalization(SAMPLE2_BaselineCorrected_df)\n",
    "SAMPLE3_Normalized_df = total_intensity_normalization(SAMPLE3_BaselineCorrected_df)\n",
    "SAMPLE4_Normalized_df = total_intensity_normalization(SAMPLE4_BaselineCorrected_df)\n",
    "SAMPLE5_Normalized_df = total_intensity_normalization(SAMPLE5_BaselineCorrected_df)\n",
    "SAMPLE6_Normalized_df = total_intensity_normalization(SAMPLE6_BaselineCorrected_df)\n",
    "SAMPLE7_Normalized_df = total_intensity_normalization(SAMPLE7_BaselineCorrected_df)\n",
    "SAMPLE8_Normalized_df = total_intensity_normalization(SAMPLE8_BaselineCorrected_df)\n",
    "SAMPLE9_Normalized_df = total_intensity_normalization(SAMPLE9_BaselineCorrected_df)\n",
    "SAMPLE10_Normalized_df = total_intensity_normalization(SAMPLE10_BaselineCorrected_df)\n",
    "SAMPLE11_Normalized_df = total_intensity_normalization(SAMPLE11_BaselineCorrected_df)\n",
    "SAMPLE12_Normalized_df = total_intensity_normalization(SAMPLE12_BaselineCorrected_df)\n",
    "SAMPLE13_Normalized_df = total_intensity_normalization(SAMPLE13_BaselineCorrected_df)\n",
    "SAMPLE14_Normalized_df = total_intensity_normalization(SAMPLE14_BaselineCorrected_df)\n",
    "SAMPLE15_Normalized_df = total_intensity_normalization(SAMPLE15_BaselineCorrected_df)\n",
    "SAMPLE16_Normalized_df = total_intensity_normalization(SAMPLE16_BaselineCorrected_df)\n",
    "SAMPLE17_Normalized_df = total_intensity_normalization(SAMPLE17_BaselineCorrected_df)\n",
    "\n",
    "\n",
    "# print(SAMPLE4_Normalized_df)\n",
    "# mean_intensity = SAMPLE7_Normalized_df['sample7_6'].mean()\n",
    "# std_intensity = SAMPLE7_Normalized_df['sample7_12'].std()\n",
    "\n",
    "# print(mean_intensity)\n",
    "# print(std_intensity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e35b5ca-144e-4c3b-8737-93383ae71217",
   "metadata": {},
   "source": [
    "3) Smoothing of the Spectrum - Now even though , the signal from the hLIBS Instrument doesn't appear to have much noise , we could still implement this additional signal smoothing steps , in our Data Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c46c4fd-ab8f-4c7f-8017-d9cc346d819a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_savitzky_golay_smoothing(df, window_length=20, polyorder=15):\n",
    "    # Selecting only the normalized intensity columns\n",
    "    normalized_intensities = df.iloc[:, 1:]\n",
    "\n",
    "    # Applying Savitzky-Golay smoothing to each intensity column\n",
    "    smoothed_intensities = normalized_intensities.apply(lambda x: savgol_filter(x, window_length, polyorder), axis=0)\n",
    "\n",
    "    # Combining wavelength column with smoothed intensities\n",
    "    smoothed_df = pd.concat([df.iloc[:, 0], smoothed_intensities], axis=1)\n",
    "\n",
    "    return smoothed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee32c802-e357-45f1-b847-e8260352c56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE1_Smoothed_df = SAMPLE1_Normalized_df\n",
    "SAMPLE2_Smoothed_df = SAMPLE2_Normalized_df\n",
    "SAMPLE3_Smoothed_df = SAMPLE3_Normalized_df\n",
    "SAMPLE4_Smoothed_df = SAMPLE4_Normalized_df\n",
    "SAMPLE5_Smoothed_df = SAMPLE5_Normalized_df\n",
    "SAMPLE6_Smoothed_df = SAMPLE6_Normalized_df\n",
    "SAMPLE7_Smoothed_df = SAMPLE7_Normalized_df\n",
    "SAMPLE8_Smoothed_df = SAMPLE8_Normalized_df\n",
    "SAMPLE9_Smoothed_df = SAMPLE9_Normalized_df\n",
    "SAMPLE10_Smoothed_df = SAMPLE10_Normalized_df\n",
    "SAMPLE11_Smoothed_df = SAMPLE11_Normalized_df\n",
    "SAMPLE12_Smoothed_df = SAMPLE12_Normalized_df\n",
    "SAMPLE13_Smoothed_df = SAMPLE13_Normalized_df\n",
    "SAMPLE14_Smoothed_df = SAMPLE14_Normalized_df\n",
    "SAMPLE15_Smoothed_df = SAMPLE15_Normalized_df\n",
    "SAMPLE16_Smoothed_df = SAMPLE16_Normalized_df\n",
    "SAMPLE17_Smoothed_df = SAMPLE17_Normalized_df\n",
    "\n",
    "\n",
    "Smoothed_Plot = figure(title = 'After Smoothing of the Spectrum' , x_axis_label = 'Wavelength' , y_axis_label = 'Intensity')\n",
    "\n",
    "Smoothed_Plot.line(SAMPLE1_Smoothed_df.wavelength,SAMPLE1_Smoothed_df.sample1_6 , line_width = 2, color =\"blue\" )\n",
    "#####################################################################################################################\n",
    "Smoothed_Plot.line(SAMPLE2_Smoothed_df.wavelength,SAMPLE2_Smoothed_df.sample2_6 , line_width = 2, color =\"orange\" )\n",
    "###############################################################################################################\n",
    "Smoothed_Plot.line(SAMPLE3_Smoothed_df.wavelength,SAMPLE3_Smoothed_df.sample3_6 , line_width = 2, color =\"green\")\n",
    "######################################################################################################################\n",
    "Smoothed_Plot.line(SAMPLE4_Smoothed_df.wavelength,SAMPLE4_Smoothed_df.sample4_6 , line_width = 2, color =\"red\")\n",
    "#############################################################################################################\n",
    "Smoothed_Plot.line(SAMPLE5_Smoothed_df.wavelength,SAMPLE5_Smoothed_df.sample5_6 , line_width = 2, color =\"purple\")\n",
    "###############################################################################################################\n",
    "Smoothed_Plot.line(SAMPLE6_Smoothed_df.wavelength,SAMPLE6_Smoothed_df.sample6_6 , line_width = 2, color =\"brown\")\n",
    "###################################################################################################################\n",
    "Smoothed_Plot.line(SAMPLE7_Smoothed_df.wavelength,SAMPLE7_Smoothed_df.sample7_6 , line_width = 2, color =\"pink\")\n",
    "##################################################################################################################\n",
    "Smoothed_Plot.line(SAMPLE8_Smoothed_df.wavelength,SAMPLE8_Smoothed_df.sample8_6 , line_width = 2, color =\"gray\")\n",
    "# ###################################################################################################################\n",
    "Smoothed_Plot.line(SAMPLE9_Smoothed_df.wavelength,SAMPLE9_Smoothed_df.sample9_6 , line_width = 2, color =\"olive\")\n",
    "###################################################################################################################\n",
    "Smoothed_Plot.line(SAMPLE10_Smoothed_df.wavelength,SAMPLE10_Smoothed_df.sample10_6 , line_width = 2, color =\"Cyan\")\n",
    "###################################################################################################################\n",
    "Smoothed_Plot.line(SAMPLE11_Smoothed_df.wavelength,SAMPLE11_Smoothed_df.sample11_6 , line_width = 2, color =\"black\")\n",
    "###################################################################################################################\n",
    "Smoothed_Plot.line(SAMPLE12_Smoothed_df.wavelength,SAMPLE12_Smoothed_df.sample12_6 , line_width = 2, color =\"tomato\")\n",
    "###################################################################################################################\n",
    "Smoothed_Plot.line(SAMPLE13_Smoothed_df.wavelength,SAMPLE13_Smoothed_df.sample13_6 , line_width = 2, color =\"steelblue\")\n",
    "###################################################################################################################\n",
    "Smoothed_Plot.line(SAMPLE14_Smoothed_df.wavelength,SAMPLE14_Smoothed_df.sample14_6 , line_width = 2, color =\"limegreen\")\n",
    "###################################################################################################################\n",
    "Smoothed_Plot.line(SAMPLE15_Smoothed_df.wavelength,SAMPLE15_Smoothed_df.sample15_6 , line_width = 2, color =\"deeppink\")\n",
    "###################################################################################################################\n",
    "Smoothed_Plot.line(SAMPLE16_Smoothed_df.wavelength,SAMPLE16_Smoothed_df.sample16_6 , line_width = 2, color =\"gold\")\n",
    "###################################################################################################################\n",
    "Smoothed_Plot.line(SAMPLE17_Smoothed_df.wavelength,SAMPLE17_Smoothed_df.sample17_6 , line_width = 2, color =\"indigo\")\n",
    "###################################################################################################################\n",
    "\n",
    "\n",
    "Smoothed_Plot.width = 900\n",
    "Smoothed_Plot.height = 500\n",
    "show(Smoothed_Plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c967b4-b009-48a6-8d67-9857cab410d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Average the Resultant Spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313409df-ca3b-427f-9228-aee74e650337",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_intensity(df, intensity_name):\n",
    "    # Select intensity columns (from column 2 to column 13)\n",
    "    intensity_columns = df.columns[1:]\n",
    "\n",
    "    # Calculate the mean of intensity columns\n",
    "    averaged_intensity = df[intensity_columns].mean(axis=1)\n",
    "\n",
    "    # Create a new DataFrame with wavelength and averaged intensity\n",
    "    averaged_df = pd.DataFrame({'wavelength': df['wavelength'], intensity_name: averaged_intensity})\n",
    "\n",
    "    return averaged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73015028-e982-40dd-bbe1-198dcf4b969a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE1_Averaged_df = average_intensity(df = SAMPLE1_Smoothed_df , intensity_name= 'sample1')\n",
    "SAMPLE2_Averaged_df = average_intensity(df = SAMPLE2_Smoothed_df , intensity_name= 'sample2')\n",
    "SAMPLE3_Averaged_df = average_intensity(df = SAMPLE3_Smoothed_df , intensity_name= 'sample3')\n",
    "SAMPLE4_Averaged_df = average_intensity(df = SAMPLE4_Smoothed_df , intensity_name= 'sample4')\n",
    "SAMPLE5_Averaged_df = average_intensity(df = SAMPLE5_Smoothed_df , intensity_name= 'sample5')\n",
    "SAMPLE6_Averaged_df = average_intensity(df = SAMPLE6_Smoothed_df , intensity_name= 'sample6')\n",
    "SAMPLE7_Averaged_df = average_intensity(df = SAMPLE7_Smoothed_df , intensity_name= 'sample7')\n",
    "SAMPLE8_Averaged_df = average_intensity(df = SAMPLE8_Smoothed_df , intensity_name= 'sample8')\n",
    "SAMPLE9_Averaged_df = average_intensity(df = SAMPLE9_Smoothed_df , intensity_name= 'sample9')\n",
    "SAMPLE10_Averaged_df = average_intensity(df = SAMPLE10_Smoothed_df , intensity_name= 'sample10')\n",
    "SAMPLE11_Averaged_df = average_intensity(df = SAMPLE11_Smoothed_df , intensity_name= 'sample11')\n",
    "SAMPLE12_Averaged_df = average_intensity(df = SAMPLE12_Smoothed_df , intensity_name= 'sample12')\n",
    "SAMPLE13_Averaged_df = average_intensity(df = SAMPLE13_Smoothed_df , intensity_name= 'sample13')\n",
    "SAMPLE14_Averaged_df = average_intensity(df = SAMPLE14_Smoothed_df , intensity_name= 'sample14')\n",
    "SAMPLE15_Averaged_df = average_intensity(df = SAMPLE15_Smoothed_df , intensity_name= 'sample15')\n",
    "SAMPLE16_Averaged_df = average_intensity(df = SAMPLE16_Smoothed_df , intensity_name= 'sample16')\n",
    "SAMPLE17_Averaged_df = average_intensity(df = SAMPLE17_Smoothed_df , intensity_name= 'sample17')\n",
    "\n",
    "print(SAMPLE17_Averaged_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b70cc1-0099-48e2-bde1-8f79c7e0d6b5",
   "metadata": {},
   "source": [
    "# Selection of ROI "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a02281c-d688-4651-9608-2461fd0c8d3e",
   "metadata": {},
   "source": [
    "Now , as we can see , we have datapoints from wavelength , in which we defined above , but our ROI is just one peak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8f0319-0924-496c-8aca-a2337604a69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Element_name = 'Ti'\n",
    "\n",
    "Peak_Pos_1 = '323.451'\n",
    "Peak_Min_1 = 323.423\n",
    "Peak_Max_1 = 323.4733\n",
    "\n",
    "Peak_Pos_2 = '337.268'\n",
    "Peak_Min_2 = 337.2452\n",
    "Peak_Max_2 = 337.2984\n",
    "\n",
    "Peak_Pos_3 = '336.118'\n",
    "Peak_Min_3 = 336.0842\n",
    "Peak_Max_3 = 336.1417\n",
    "\n",
    "Peak_Pos_4 = '279.475'\n",
    "Peak_Min_4 = 279.451\n",
    "Peak_Max_4 = 279.495\n",
    "\n",
    "SAMPLE1_Peak1 = SAMPLE1_Smoothed_df[(SAMPLE1_Smoothed_df['wavelength'] >= Peak_Min_1) & (SAMPLE1_Smoothed_df['wavelength'] <= Peak_Max_1)]\n",
    "SAMPLE2_Peak1 = SAMPLE2_Smoothed_df[(SAMPLE2_Smoothed_df['wavelength'] >= Peak_Min_1) & (SAMPLE2_Smoothed_df['wavelength'] <= Peak_Max_1)]\n",
    "SAMPLE3_Peak1 = SAMPLE3_Smoothed_df[(SAMPLE3_Smoothed_df['wavelength'] >= Peak_Min_1) & (SAMPLE3_Smoothed_df['wavelength'] <= Peak_Max_1)]\n",
    "SAMPLE4_Peak1 = SAMPLE4_Smoothed_df[(SAMPLE4_Smoothed_df['wavelength'] >= Peak_Min_1) & (SAMPLE4_Smoothed_df['wavelength'] <= Peak_Max_1)]\n",
    "SAMPLE5_Peak1 = SAMPLE5_Smoothed_df[(SAMPLE5_Smoothed_df['wavelength'] >= Peak_Min_1) & (SAMPLE5_Smoothed_df['wavelength'] <= Peak_Max_1)]\n",
    "SAMPLE6_Peak1 = SAMPLE6_Smoothed_df[(SAMPLE6_Smoothed_df['wavelength'] >= Peak_Min_1) & (SAMPLE6_Smoothed_df['wavelength'] <= Peak_Max_1)]\n",
    "SAMPLE7_Peak1 = SAMPLE7_Smoothed_df[(SAMPLE7_Smoothed_df['wavelength'] >= Peak_Min_1) & (SAMPLE7_Smoothed_df['wavelength'] <= Peak_Max_1)]\n",
    "SAMPLE8_Peak1 = SAMPLE8_Smoothed_df[(SAMPLE8_Smoothed_df['wavelength'] >= Peak_Min_1) & (SAMPLE8_Smoothed_df['wavelength'] <= Peak_Max_1)]\n",
    "SAMPLE9_Peak1 = SAMPLE9_Smoothed_df[(SAMPLE9_Smoothed_df['wavelength'] >= Peak_Min_1) & (SAMPLE9_Smoothed_df['wavelength'] <= Peak_Max_1)]\n",
    "SAMPLE10_Peak1 = SAMPLE10_Smoothed_df[(SAMPLE10_Smoothed_df['wavelength'] >= Peak_Min_1) & (SAMPLE10_Smoothed_df['wavelength'] <= Peak_Max_1)]\n",
    "SAMPLE11_Peak1 = SAMPLE11_Smoothed_df[(SAMPLE11_Smoothed_df['wavelength'] >= Peak_Min_1) & (SAMPLE11_Smoothed_df['wavelength'] <= Peak_Max_1)]\n",
    "SAMPLE12_Peak1 = SAMPLE12_Smoothed_df[(SAMPLE12_Smoothed_df['wavelength'] >= Peak_Min_1) & (SAMPLE12_Smoothed_df['wavelength'] <= Peak_Max_1)]\n",
    "SAMPLE13_Peak1 = SAMPLE13_Smoothed_df[(SAMPLE13_Smoothed_df['wavelength'] >= Peak_Min_1) & (SAMPLE13_Smoothed_df['wavelength'] <= Peak_Max_1)]\n",
    "SAMPLE14_Peak1 = SAMPLE14_Smoothed_df[(SAMPLE14_Smoothed_df['wavelength'] >= Peak_Min_1) & (SAMPLE14_Smoothed_df['wavelength'] <= Peak_Max_1)]\n",
    "SAMPLE15_Peak1 = SAMPLE15_Smoothed_df[(SAMPLE15_Smoothed_df['wavelength'] >= Peak_Min_1) & (SAMPLE15_Smoothed_df['wavelength'] <= Peak_Max_1)]\n",
    "SAMPLE16_Peak1 = SAMPLE16_Smoothed_df[(SAMPLE16_Smoothed_df['wavelength'] >= Peak_Min_1) & (SAMPLE16_Smoothed_df['wavelength'] <= Peak_Max_1)]\n",
    "SAMPLE17_Peak1 = SAMPLE17_Smoothed_df[(SAMPLE17_Smoothed_df['wavelength'] >= Peak_Min_1) & (SAMPLE17_Smoothed_df['wavelength'] <= Peak_Max_1)]\n",
    "\n",
    "\n",
    "SAMPLE1_Peak2 = SAMPLE1_Smoothed_df[(SAMPLE1_Smoothed_df['wavelength'] >= Peak_Min_2) & (SAMPLE1_Smoothed_df['wavelength'] <= Peak_Max_2)]\n",
    "SAMPLE2_Peak2 = SAMPLE2_Smoothed_df[(SAMPLE2_Smoothed_df['wavelength'] >= Peak_Min_2) & (SAMPLE2_Smoothed_df['wavelength'] <= Peak_Max_2)]\n",
    "SAMPLE3_Peak2 = SAMPLE3_Smoothed_df[(SAMPLE3_Smoothed_df['wavelength'] >= Peak_Min_2) & (SAMPLE3_Smoothed_df['wavelength'] <= Peak_Max_2)]\n",
    "SAMPLE4_Peak2 = SAMPLE4_Smoothed_df[(SAMPLE4_Smoothed_df['wavelength'] >= Peak_Min_2) & (SAMPLE4_Smoothed_df['wavelength'] <= Peak_Max_2)]\n",
    "SAMPLE5_Peak2 = SAMPLE5_Smoothed_df[(SAMPLE5_Smoothed_df['wavelength'] >= Peak_Min_2) & (SAMPLE5_Smoothed_df['wavelength'] <= Peak_Max_2)]\n",
    "SAMPLE6_Peak2 = SAMPLE6_Smoothed_df[(SAMPLE6_Smoothed_df['wavelength'] >= Peak_Min_2) & (SAMPLE6_Smoothed_df['wavelength'] <= Peak_Max_2)]\n",
    "SAMPLE7_Peak2 = SAMPLE7_Smoothed_df[(SAMPLE7_Smoothed_df['wavelength'] >= Peak_Min_2) & (SAMPLE7_Smoothed_df['wavelength'] <= Peak_Max_2)]\n",
    "SAMPLE8_Peak2 = SAMPLE8_Smoothed_df[(SAMPLE8_Smoothed_df['wavelength'] >= Peak_Min_2) & (SAMPLE8_Smoothed_df['wavelength'] <= Peak_Max_2)]\n",
    "SAMPLE9_Peak2 = SAMPLE9_Smoothed_df[(SAMPLE9_Smoothed_df['wavelength'] >= Peak_Min_2) & (SAMPLE9_Smoothed_df['wavelength'] <= Peak_Max_2)]\n",
    "SAMPLE10_Peak2 = SAMPLE10_Smoothed_df[(SAMPLE10_Smoothed_df['wavelength'] >= Peak_Min_2) & (SAMPLE10_Smoothed_df['wavelength'] <= Peak_Max_2)]\n",
    "SAMPLE11_Peak2 = SAMPLE11_Smoothed_df[(SAMPLE11_Smoothed_df['wavelength'] >= Peak_Min_2) & (SAMPLE11_Smoothed_df['wavelength'] <= Peak_Max_2)]\n",
    "SAMPLE12_Peak2 = SAMPLE12_Smoothed_df[(SAMPLE12_Smoothed_df['wavelength'] >= Peak_Min_2) & (SAMPLE12_Smoothed_df['wavelength'] <= Peak_Max_2)]\n",
    "SAMPLE13_Peak2 = SAMPLE13_Smoothed_df[(SAMPLE13_Smoothed_df['wavelength'] >= Peak_Min_2) & (SAMPLE13_Smoothed_df['wavelength'] <= Peak_Max_2)]\n",
    "SAMPLE14_Peak2 = SAMPLE14_Smoothed_df[(SAMPLE14_Smoothed_df['wavelength'] >= Peak_Min_2) & (SAMPLE14_Smoothed_df['wavelength'] <= Peak_Max_2)]\n",
    "SAMPLE15_Peak2 = SAMPLE15_Smoothed_df[(SAMPLE15_Smoothed_df['wavelength'] >= Peak_Min_2) & (SAMPLE15_Smoothed_df['wavelength'] <= Peak_Max_2)]\n",
    "SAMPLE16_Peak2 = SAMPLE16_Smoothed_df[(SAMPLE16_Smoothed_df['wavelength'] >= Peak_Min_2) & (SAMPLE16_Smoothed_df['wavelength'] <= Peak_Max_2)]\n",
    "SAMPLE17_Peak2 = SAMPLE17_Smoothed_df[(SAMPLE17_Smoothed_df['wavelength'] >= Peak_Min_2) & (SAMPLE17_Smoothed_df['wavelength'] <= Peak_Max_2)]\n",
    "\n",
    "SAMPLE1_Peak3 = SAMPLE1_Smoothed_df[(SAMPLE1_Smoothed_df['wavelength'] >= Peak_Min_3) & (SAMPLE1_Smoothed_df['wavelength'] <= Peak_Max_3)]\n",
    "SAMPLE2_Peak3 = SAMPLE2_Smoothed_df[(SAMPLE2_Smoothed_df['wavelength'] >= Peak_Min_3) & (SAMPLE2_Smoothed_df['wavelength'] <= Peak_Max_3)]\n",
    "SAMPLE3_Peak3 = SAMPLE3_Smoothed_df[(SAMPLE3_Smoothed_df['wavelength'] >= Peak_Min_3) & (SAMPLE3_Smoothed_df['wavelength'] <= Peak_Max_3)]\n",
    "SAMPLE4_Peak3 = SAMPLE4_Smoothed_df[(SAMPLE4_Smoothed_df['wavelength'] >= Peak_Min_3) & (SAMPLE4_Smoothed_df['wavelength'] <= Peak_Max_3)]\n",
    "SAMPLE5_Peak3 = SAMPLE5_Smoothed_df[(SAMPLE5_Smoothed_df['wavelength'] >= Peak_Min_3) & (SAMPLE5_Smoothed_df['wavelength'] <= Peak_Max_3)]\n",
    "SAMPLE6_Peak3 = SAMPLE6_Smoothed_df[(SAMPLE6_Smoothed_df['wavelength'] >= Peak_Min_3) & (SAMPLE6_Smoothed_df['wavelength'] <= Peak_Max_3)]\n",
    "SAMPLE7_Peak3 = SAMPLE7_Smoothed_df[(SAMPLE7_Smoothed_df['wavelength'] >= Peak_Min_3) & (SAMPLE7_Smoothed_df['wavelength'] <= Peak_Max_3)]\n",
    "SAMPLE8_Peak3 = SAMPLE8_Smoothed_df[(SAMPLE8_Smoothed_df['wavelength'] >= Peak_Min_3) & (SAMPLE8_Smoothed_df['wavelength'] <= Peak_Max_3)]\n",
    "SAMPLE9_Peak3 = SAMPLE9_Smoothed_df[(SAMPLE9_Smoothed_df['wavelength'] >= Peak_Min_3) & (SAMPLE9_Smoothed_df['wavelength'] <= Peak_Max_3)]\n",
    "SAMPLE10_Peak3 = SAMPLE10_Smoothed_df[(SAMPLE10_Smoothed_df['wavelength'] >= Peak_Min_3) & (SAMPLE10_Smoothed_df['wavelength'] <= Peak_Max_3)]\n",
    "SAMPLE11_Peak3 = SAMPLE11_Smoothed_df[(SAMPLE11_Smoothed_df['wavelength'] >= Peak_Min_3) & (SAMPLE11_Smoothed_df['wavelength'] <= Peak_Max_3)]\n",
    "SAMPLE12_Peak3 = SAMPLE12_Smoothed_df[(SAMPLE12_Smoothed_df['wavelength'] >= Peak_Min_3) & (SAMPLE12_Smoothed_df['wavelength'] <= Peak_Max_3)]\n",
    "SAMPLE13_Peak3 = SAMPLE13_Smoothed_df[(SAMPLE13_Smoothed_df['wavelength'] >= Peak_Min_3) & (SAMPLE13_Smoothed_df['wavelength'] <= Peak_Max_3)]\n",
    "SAMPLE14_Peak3 = SAMPLE14_Smoothed_df[(SAMPLE14_Smoothed_df['wavelength'] >= Peak_Min_3) & (SAMPLE14_Smoothed_df['wavelength'] <= Peak_Max_3)]\n",
    "SAMPLE15_Peak3 = SAMPLE15_Smoothed_df[(SAMPLE15_Smoothed_df['wavelength'] >= Peak_Min_3) & (SAMPLE15_Smoothed_df['wavelength'] <= Peak_Max_3)]\n",
    "SAMPLE16_Peak3 = SAMPLE16_Smoothed_df[(SAMPLE16_Smoothed_df['wavelength'] >= Peak_Min_3) & (SAMPLE16_Smoothed_df['wavelength'] <= Peak_Max_3)]\n",
    "SAMPLE17_Peak3 = SAMPLE17_Smoothed_df[(SAMPLE17_Smoothed_df['wavelength'] >= Peak_Min_3) & (SAMPLE17_Smoothed_df['wavelength'] <= Peak_Max_3)]\n",
    "\n",
    "SAMPLE1_Peak4 = SAMPLE1_Smoothed_df[(SAMPLE1_Smoothed_df['wavelength'] >= Peak_Min_4) & (SAMPLE1_Smoothed_df['wavelength'] <= Peak_Max_4)]\n",
    "SAMPLE2_Peak4 = SAMPLE2_Smoothed_df[(SAMPLE2_Smoothed_df['wavelength'] >= Peak_Min_4) & (SAMPLE2_Smoothed_df['wavelength'] <= Peak_Max_4)]\n",
    "SAMPLE3_Peak4 = SAMPLE3_Smoothed_df[(SAMPLE3_Smoothed_df['wavelength'] >= Peak_Min_4) & (SAMPLE3_Smoothed_df['wavelength'] <= Peak_Max_4)]\n",
    "SAMPLE4_Peak4 = SAMPLE4_Smoothed_df[(SAMPLE4_Smoothed_df['wavelength'] >= Peak_Min_4) & (SAMPLE4_Smoothed_df['wavelength'] <= Peak_Max_4)]\n",
    "SAMPLE5_Peak4 = SAMPLE5_Smoothed_df[(SAMPLE5_Smoothed_df['wavelength'] >= Peak_Min_4) & (SAMPLE5_Smoothed_df['wavelength'] <= Peak_Max_4)]\n",
    "SAMPLE6_Peak4 = SAMPLE6_Smoothed_df[(SAMPLE6_Smoothed_df['wavelength'] >= Peak_Min_4) & (SAMPLE6_Smoothed_df['wavelength'] <= Peak_Max_4)]\n",
    "SAMPLE7_Peak4 = SAMPLE7_Smoothed_df[(SAMPLE7_Smoothed_df['wavelength'] >= Peak_Min_4) & (SAMPLE7_Smoothed_df['wavelength'] <= Peak_Max_4)]\n",
    "SAMPLE8_Peak4 = SAMPLE8_Smoothed_df[(SAMPLE8_Smoothed_df['wavelength'] >= Peak_Min_4) & (SAMPLE8_Smoothed_df['wavelength'] <= Peak_Max_4)]\n",
    "SAMPLE9_Peak4 = SAMPLE9_Smoothed_df[(SAMPLE9_Smoothed_df['wavelength'] >= Peak_Min_4) & (SAMPLE9_Smoothed_df['wavelength'] <= Peak_Max_4)]\n",
    "SAMPLE10_Peak4 = SAMPLE10_Smoothed_df[(SAMPLE10_Smoothed_df['wavelength'] >= Peak_Min_4) & (SAMPLE10_Smoothed_df['wavelength'] <= Peak_Max_4)]\n",
    "SAMPLE11_Peak4 = SAMPLE11_Smoothed_df[(SAMPLE11_Smoothed_df['wavelength'] >= Peak_Min_4) & (SAMPLE11_Smoothed_df['wavelength'] <= Peak_Max_4)]\n",
    "SAMPLE12_Peak4 = SAMPLE12_Smoothed_df[(SAMPLE12_Smoothed_df['wavelength'] >= Peak_Min_4) & (SAMPLE12_Smoothed_df['wavelength'] <= Peak_Max_4)]\n",
    "SAMPLE13_Peak4 = SAMPLE13_Smoothed_df[(SAMPLE13_Smoothed_df['wavelength'] >= Peak_Min_4) & (SAMPLE13_Smoothed_df['wavelength'] <= Peak_Max_4)]\n",
    "SAMPLE14_Peak4 = SAMPLE14_Smoothed_df[(SAMPLE14_Smoothed_df['wavelength'] >= Peak_Min_4) & (SAMPLE14_Smoothed_df['wavelength'] <= Peak_Max_4)]\n",
    "SAMPLE15_Peak4 = SAMPLE15_Smoothed_df[(SAMPLE15_Smoothed_df['wavelength'] >= Peak_Min_4) & (SAMPLE15_Smoothed_df['wavelength'] <= Peak_Max_4)]\n",
    "SAMPLE16_Peak4 = SAMPLE16_Smoothed_df[(SAMPLE16_Smoothed_df['wavelength'] >= Peak_Min_4) & (SAMPLE16_Smoothed_df['wavelength'] <= Peak_Max_4)]\n",
    "SAMPLE17_Peak4 = SAMPLE17_Smoothed_df[(SAMPLE17_Smoothed_df['wavelength'] >= Peak_Min_4) & (SAMPLE17_Smoothed_df['wavelength'] <= Peak_Max_4)]\n",
    "\n",
    "SAMPLE1_Peak1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d4fea7-a8f3-4274-b4b1-d48cb62bd587",
   "metadata": {},
   "source": [
    "# Gaussian Fit and Outlier Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ce6ed9-7b09-41ba-9730-62928ae6cbce",
   "metadata": {},
   "source": [
    "As you see from the Pic above , even after normalizing the Spectra , there are few variations in the Intensities , Before passing them to regression step , the important step would be to do Outlier Removal of the Intensities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9d0133-3de9-4ea4-ba92-56f42c34b44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "# Define the Gaussian profile\n",
    "def gaussian(x, center, amplitude, sigma):\n",
    "    return amplitude * np.exp(-((x - center)**2) / (2 * sigma**2))\n",
    "\n",
    "def fit_gaussian_profile(df):\n",
    "    # Extract the wavelength data\n",
    "    wavelength = df.iloc[:, 0].values\n",
    "\n",
    "    # Prepare to store the fitted profiles\n",
    "    fitted_data = pd.DataFrame({'wavelength': wavelength})\n",
    "\n",
    "    # Iterate over each intensity column\n",
    "    for column in df.columns[1:]:\n",
    "        intensity = df[column].values\n",
    "\n",
    "        # Find the peak\n",
    "        peak_indices, _ = find_peaks(intensity)\n",
    "        if len(peak_indices) == 0:\n",
    "            raise ValueError(f\"No peaks found in the data for column {column}.\")\n",
    "        \n",
    "        peak_index = peak_indices[np.argmax(intensity[peak_indices])]\n",
    "        center_guess = wavelength[peak_index]\n",
    "        amplitude_guess = intensity[peak_index]\n",
    "\n",
    "        # Find FWHM\n",
    "        half_max = amplitude_guess / 2\n",
    "        indices_above_half_max = np.where(intensity >= half_max)[0]\n",
    "\n",
    "        if len(indices_above_half_max) >= 2:\n",
    "            # Interpolate to find the exact FWHM\n",
    "            left_index = indices_above_half_max[0]\n",
    "            right_index = indices_above_half_max[-1]\n",
    "            fwhm = wavelength[right_index] - wavelength[left_index]\n",
    "\n",
    "            # Convert FWHM to sigma\n",
    "            sigma_guess = fwhm / (2 * np.sqrt(2 * np.log(2)))\n",
    "        else:\n",
    "            # Fallback to the range-based estimate if FWHM calculation fails\n",
    "            sigma_guess = (wavelength[-1] - wavelength[0]) / 10\n",
    "\n",
    "        # Initial guesses for the fit parameters\n",
    "        initial_guesses = [center_guess, amplitude_guess, sigma_guess]\n",
    "\n",
    "        # Fit the Gaussian profile to the data\n",
    "        popt, _ = curve_fit(gaussian, wavelength, intensity, p0=initial_guesses)\n",
    "\n",
    "        # Generate the fitted Gaussian profile using the best fit parameters\n",
    "        fitted_profile = gaussian(wavelength, *popt)\n",
    "\n",
    "        # Add the fitted profile to the DataFrame\n",
    "        fitted_data[column] = fitted_profile\n",
    "\n",
    "    return fitted_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243acf9b-c4b3-4f25-a445-9d7ed3ef0875",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE1_Peak1_fit = fit_gaussian_profile(SAMPLE1_Peak1)\n",
    "SAMPLE2_Peak1_fit = fit_gaussian_profile(SAMPLE2_Peak1)\n",
    "SAMPLE3_Peak1_fit = (SAMPLE3_Peak1)\n",
    "SAMPLE4_Peak1_fit = fit_gaussian_profile(SAMPLE4_Peak1)\n",
    "SAMPLE5_Peak1_fit = fit_gaussian_profile(SAMPLE5_Peak1)\n",
    "SAMPLE6_Peak1_fit = fit_gaussian_profile(SAMPLE6_Peak1)\n",
    "SAMPLE7_Peak1_fit = fit_gaussian_profile(SAMPLE7_Peak1)\n",
    "SAMPLE8_Peak1_fit = fit_gaussian_profile(SAMPLE8_Peak1)\n",
    "SAMPLE9_Peak1_fit = fit_gaussian_profile(SAMPLE9_Peak1)\n",
    "SAMPLE10_Peak1_fit = fit_gaussian_profile(SAMPLE10_Peak1)\n",
    "SAMPLE11_Peak1_fit = fit_gaussian_profile(SAMPLE11_Peak1)\n",
    "SAMPLE12_Peak1_fit = (SAMPLE12_Peak1)\n",
    "SAMPLE13_Peak1_fit =(SAMPLE13_Peak1)\n",
    "SAMPLE14_Peak1_fit = fit_gaussian_profile(SAMPLE14_Peak1)\n",
    "SAMPLE15_Peak1_fit = fit_gaussian_profile(SAMPLE15_Peak1)\n",
    "SAMPLE16_Peak1_fit = fit_gaussian_profile(SAMPLE16_Peak1)\n",
    "SAMPLE17_Peak1_fit =(SAMPLE17_Peak1)\n",
    "\n",
    "SAMPLE1_Peak2_fit = fit_gaussian_profile(SAMPLE1_Peak2)\n",
    "SAMPLE2_Peak2_fit = fit_gaussian_profile(SAMPLE2_Peak2)\n",
    "SAMPLE3_Peak2_fit = (SAMPLE3_Peak2)\n",
    "SAMPLE4_Peak2_fit = fit_gaussian_profile(SAMPLE4_Peak2)\n",
    "SAMPLE5_Peak2_fit = fit_gaussian_profile(SAMPLE5_Peak2)\n",
    "SAMPLE6_Peak2_fit = fit_gaussian_profile(SAMPLE6_Peak2)\n",
    "SAMPLE7_Peak2_fit = fit_gaussian_profile(SAMPLE7_Peak2)\n",
    "SAMPLE8_Peak2_fit = fit_gaussian_profile(SAMPLE8_Peak2)\n",
    "SAMPLE9_Peak2_fit = fit_gaussian_profile(SAMPLE9_Peak2)\n",
    "SAMPLE10_Peak2_fit = fit_gaussian_profile(SAMPLE10_Peak2)\n",
    "SAMPLE11_Peak2_fit = fit_gaussian_profile(SAMPLE11_Peak2)\n",
    "SAMPLE12_Peak2_fit = (SAMPLE12_Peak2)\n",
    "SAMPLE13_Peak2_fit = (SAMPLE13_Peak2)\n",
    "SAMPLE14_Peak2_fit = fit_gaussian_profile(SAMPLE14_Peak2)\n",
    "SAMPLE15_Peak2_fit = fit_gaussian_profile(SAMPLE15_Peak2)\n",
    "SAMPLE16_Peak2_fit = fit_gaussian_profile(SAMPLE16_Peak2)\n",
    "SAMPLE17_Peak2_fit = (SAMPLE17_Peak2)\n",
    "\n",
    "SAMPLE1_Peak3_fit = fit_gaussian_profile(SAMPLE1_Peak3)\n",
    "SAMPLE2_Peak3_fit = fit_gaussian_profile(SAMPLE2_Peak3)\n",
    "SAMPLE3_Peak3_fit = (SAMPLE3_Peak3)\n",
    "SAMPLE4_Peak3_fit = fit_gaussian_profile(SAMPLE4_Peak3)\n",
    "SAMPLE5_Peak3_fit = fit_gaussian_profile(SAMPLE5_Peak3)\n",
    "SAMPLE6_Peak3_fit = fit_gaussian_profile(SAMPLE6_Peak3)\n",
    "SAMPLE7_Peak3_fit = fit_gaussian_profile(SAMPLE7_Peak3)\n",
    "SAMPLE8_Peak3_fit = fit_gaussian_profile(SAMPLE8_Peak3)\n",
    "SAMPLE9_Peak3_fit = fit_gaussian_profile(SAMPLE9_Peak3)\n",
    "SAMPLE10_Peak3_fit = fit_gaussian_profile(SAMPLE10_Peak3)\n",
    "SAMPLE11_Peak3_fit = fit_gaussian_profile(SAMPLE11_Peak3)\n",
    "SAMPLE12_Peak3_fit = (SAMPLE12_Peak3)\n",
    "SAMPLE13_Peak3_fit = (SAMPLE13_Peak3)\n",
    "SAMPLE14_Peak3_fit = fit_gaussian_profile(SAMPLE14_Peak3)\n",
    "SAMPLE15_Peak3_fit = fit_gaussian_profile(SAMPLE15_Peak3)\n",
    "SAMPLE16_Peak3_fit = fit_gaussian_profile(SAMPLE16_Peak3)\n",
    "SAMPLE17_Peak3_fit = (SAMPLE17_Peak3)\n",
    "\n",
    "SAMPLE1_Peak4_fit = fit_gaussian_profile(SAMPLE1_Peak4)\n",
    "SAMPLE2_Peak4_fit = fit_gaussian_profile(SAMPLE2_Peak4)\n",
    "SAMPLE3_Peak4_fit = fit_gaussian_profile(SAMPLE3_Peak4)\n",
    "SAMPLE4_Peak4_fit = fit_gaussian_profile(SAMPLE4_Peak4)\n",
    "SAMPLE5_Peak4_fit = fit_gaussian_profile(SAMPLE5_Peak4)\n",
    "SAMPLE6_Peak4_fit = fit_gaussian_profile(SAMPLE6_Peak4)\n",
    "SAMPLE7_Peak4_fit = fit_gaussian_profile(SAMPLE7_Peak4)\n",
    "SAMPLE8_Peak4_fit = fit_gaussian_profile(SAMPLE8_Peak4)\n",
    "SAMPLE9_Peak4_fit = fit_gaussian_profile(SAMPLE9_Peak4)\n",
    "SAMPLE10_Peak4_fit = fit_gaussian_profile(SAMPLE10_Peak4)\n",
    "SAMPLE11_Peak4_fit = fit_gaussian_profile(SAMPLE11_Peak4)\n",
    "SAMPLE12_Peak4_fit = fit_gaussian_profile(SAMPLE12_Peak4)\n",
    "SAMPLE13_Peak4_fit = fit_gaussian_profile(SAMPLE13_Peak4)\n",
    "SAMPLE14_Peak4_fit = fit_gaussian_profile(SAMPLE14_Peak4)\n",
    "SAMPLE15_Peak4_fit = fit_gaussian_profile(SAMPLE15_Peak4)\n",
    "SAMPLE16_Peak4_fit = fit_gaussian_profile(SAMPLE16_Peak4)\n",
    "SAMPLE17_Peak4_fit = fit_gaussian_profile(SAMPLE17_Peak4)\n",
    "\n",
    "print(SAMPLE2_Peak1_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f67df2-75da-4b71-8c96-6ae7526a130c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_intensities(processed_df):\n",
    "    all_intensities = []\n",
    "\n",
    "    for col in processed_df.columns[1:]: #PEAK EXTRACTION\n",
    "        max_intensity = processed_df[col].max()\n",
    "        min_intensity = processed_df[col].min()\n",
    "        all_intensities.append(max_intensity)\n",
    "\n",
    "    all_intensities = np.array(all_intensities)\n",
    "    mean_intensities = np.mean(all_intensities)\n",
    "    std_intensities = np.std(all_intensities)\n",
    "    z_scores = np.abs((all_intensities - mean_intensities) / std_intensities)\n",
    "\n",
    "    cleaned_intensities = [intensity for intensity, z_score in zip(all_intensities, z_scores) if z_score < 3]\n",
    "    \n",
    "    mean_cleaned_intensities = np.mean(cleaned_intensities)\n",
    "\n",
    "    return mean_cleaned_intensities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c23c188-7542-4ba9-bcf5-9aa71b30723f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# def calculate_peak_area_fwhm(signal, x_values):\n",
    "#     \"\"\"Calculate the area under the peak between the Full Width at Half Maximum (FWHM).\"\"\"\n",
    "#     # Find the maximum value of the signal\n",
    "#     max_value = np.max(signal)\n",
    "#     half_max = max_value / 2\n",
    "    \n",
    "#     # Find indices where the signal crosses half the maximum value\n",
    "#     above_half_max = signal >= half_max\n",
    "#     crossing_points = np.where(above_half_max)[0]\n",
    "    \n",
    "#     if len(crossing_points) < 2:\n",
    "#         # Not enough points to define FWHM\n",
    "#         return 0, None, None\n",
    "\n",
    "#     # Determine FWHM indices\n",
    "#     fwhm_start_idx = crossing_points[0]\n",
    "#     fwhm_end_idx = crossing_points[-1]\n",
    "    \n",
    "#     # Extract the portion of the signal between the FWHM points\n",
    "#     fwhm_signal = signal[fwhm_start_idx:fwhm_end_idx+1]\n",
    "#     fwhm_x_values = x_values[fwhm_start_idx:fwhm_end_idx+1]\n",
    "    \n",
    "#     # Calculate the area under the curve using the trapezoidal rule\n",
    "#     peak_area = np.trapz(fwhm_signal, fwhm_x_values)\n",
    "    \n",
    "#     return peak_area, fwhm_x_values, fwhm_signal\n",
    "\n",
    "# def clean_intensities(processed_df):\n",
    "#     all_areas = []\n",
    "\n",
    "#     # first column is not part of the signal data (e.g., time or index)\n",
    "#     x_values = processed_df.iloc[:, 0].values\n",
    "\n",
    "#     for col in processed_df.columns[1:]:\n",
    "#         signal = processed_df[col].values\n",
    "#         peak_area, fwhm_x_values, fwhm_signal = calculate_peak_area_fwhm(signal, x_values)\n",
    "#         all_areas.append(peak_area)\n",
    "\n",
    "        # Plotting the signal and FWHM\n",
    "        # plt.figure(figsize=(10, 6))\n",
    "        # plt.plot(x_values, signal, label='Signal')\n",
    "        # if fwhm_x_values is not None and fwhm_signal is not None:\n",
    "        #     plt.fill_between(fwhm_x_values, fwhm_signal, alpha=0.3, label='FWHM Area')\n",
    "        #     plt.axhline(y=np.max(signal) / 2, color='r', linestyle='--', label='FWHM Half-Max')\n",
    "        # plt.xlabel('Wavelength')\n",
    "        # plt.ylabel('Intensity')\n",
    "        # plt.title(f'Signal and FWHM Area for {col}')\n",
    "        # plt.legend()\n",
    "        # # plt.show()\n",
    "\n",
    "    # all_areas = np.array(all_areas)\n",
    "    # mean_areas = np.mean(all_areas)\n",
    "    # std_areas = np.std(all_areas)\n",
    "    # z_scores = np.abs((all_areas - mean_areas) / std_areas)\n",
    "\n",
    "    # cleaned_areas = [area for area, z_score in zip(all_areas, z_scores) if z_score < 3]\n",
    "    # mean_cleaned_areas = np.mean(cleaned_areas)\n",
    "\n",
    "    # return mean_cleaned_areas\n",
    "\n",
    "# Example usage:\n",
    "# processed_df = pd.DataFrame({'Time': time_data, 'Signal1': signal1_data, 'Signal2': signal2_data, ...})\n",
    "# cleaned_areas, all_areas = clean_intensities(processed_df, threshold=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce896d4-e345-4ec1-826c-7d2af0298ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def clean_intensities_all(processed_df):\n",
    "#     all_intensities = []\n",
    "\n",
    "#     for col in processed_df.columns[1:]: #PEAK EXTRACTION\n",
    "#         max_intensity = processed_df[col].max()\n",
    "#         min_intensity = processed_df[col].min()\n",
    "#         all_intensities.append(max_intensity)\n",
    "\n",
    "#     all_intensities = np.array(all_intensities)\n",
    "#     mean_intensities = np.mean(all_intensities)\n",
    "#     std_intensities = np.std(all_intensities)\n",
    "#     z_scores = np.abs((all_intensities - mean_intensities) / std_intensities)\n",
    "\n",
    "#     cleaned_intensities = [intensity for intensity, z_score in zip(all_intensities, z_scores) if z_score < 3]\n",
    "    \n",
    "#     mean_cleaned_intensities = np.mean(cleaned_intensities)\n",
    "\n",
    "#     return cleaned_intensities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff12660b-a97a-4d24-ae02-2ac410dcd8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE1_Peak1_Selected_Intensities = clean_intensities(processed_df=SAMPLE1_Peak1_fit)\n",
    "SAMPLE2_Peak1_Selected_Intensities = clean_intensities(processed_df=SAMPLE2_Peak1_fit)\n",
    "SAMPLE3_Peak1_Selected_Intensities = clean_intensities(processed_df=SAMPLE3_Peak1_fit)\n",
    "SAMPLE4_Peak1_Selected_Intensities = clean_intensities(processed_df=SAMPLE4_Peak1_fit)\n",
    "SAMPLE5_Peak1_Selected_Intensities = clean_intensities(processed_df=SAMPLE5_Peak1_fit)\n",
    "SAMPLE6_Peak1_Selected_Intensities = clean_intensities(processed_df=SAMPLE6_Peak1_fit)\n",
    "SAMPLE7_Peak1_Selected_Intensities = clean_intensities(processed_df=SAMPLE7_Peak1_fit)\n",
    "SAMPLE8_Peak1_Selected_Intensities = clean_intensities(processed_df=SAMPLE8_Peak1_fit)\n",
    "SAMPLE9_Peak1_Selected_Intensities = clean_intensities(processed_df=SAMPLE9_Peak1_fit)\n",
    "SAMPLE10_Peak1_Selected_Intensities = clean_intensities(processed_df=SAMPLE10_Peak1_fit)\n",
    "SAMPLE11_Peak1_Selected_Intensities = clean_intensities(processed_df=SAMPLE11_Peak1_fit)\n",
    "SAMPLE12_Peak1_Selected_Intensities = clean_intensities(processed_df=SAMPLE12_Peak1_fit)\n",
    "SAMPLE13_Peak1_Selected_Intensities = clean_intensities(processed_df=SAMPLE13_Peak1_fit)\n",
    "SAMPLE14_Peak1_Selected_Intensities = clean_intensities(processed_df=SAMPLE14_Peak1_fit)\n",
    "SAMPLE15_Peak1_Selected_Intensities = clean_intensities(processed_df=SAMPLE15_Peak1_fit)\n",
    "SAMPLE16_Peak1_Selected_Intensities = clean_intensities(processed_df=SAMPLE16_Peak1_fit)\n",
    "SAMPLE17_Peak1_Selected_Intensities = clean_intensities(processed_df=SAMPLE17_Peak1_fit)\n",
    "\n",
    "\n",
    "SAMPLE1_Peak2_Selected_Intensities = clean_intensities(processed_df=SAMPLE1_Peak2_fit)\n",
    "SAMPLE2_Peak2_Selected_Intensities = clean_intensities(processed_df=SAMPLE2_Peak2_fit)\n",
    "SAMPLE3_Peak2_Selected_Intensities = clean_intensities(processed_df=SAMPLE3_Peak2_fit)\n",
    "SAMPLE4_Peak2_Selected_Intensities = clean_intensities(processed_df=SAMPLE4_Peak2_fit)\n",
    "SAMPLE5_Peak2_Selected_Intensities = clean_intensities(processed_df=SAMPLE5_Peak2_fit)\n",
    "SAMPLE6_Peak2_Selected_Intensities = clean_intensities(processed_df=SAMPLE6_Peak2_fit)\n",
    "SAMPLE7_Peak2_Selected_Intensities = clean_intensities(processed_df=SAMPLE7_Peak2_fit)\n",
    "SAMPLE8_Peak2_Selected_Intensities = clean_intensities(processed_df=SAMPLE8_Peak2_fit)\n",
    "SAMPLE9_Peak2_Selected_Intensities = clean_intensities(processed_df=SAMPLE9_Peak2_fit)\n",
    "SAMPLE10_Peak2_Selected_Intensities = clean_intensities(processed_df=SAMPLE10_Peak2_fit)\n",
    "SAMPLE11_Peak2_Selected_Intensities = clean_intensities(processed_df=SAMPLE11_Peak2_fit)\n",
    "SAMPLE12_Peak2_Selected_Intensities = clean_intensities(processed_df=SAMPLE12_Peak2_fit)\n",
    "SAMPLE13_Peak2_Selected_Intensities = clean_intensities(processed_df=SAMPLE13_Peak2_fit)\n",
    "SAMPLE14_Peak2_Selected_Intensities = clean_intensities(processed_df=SAMPLE14_Peak2_fit)\n",
    "SAMPLE15_Peak2_Selected_Intensities = clean_intensities(processed_df=SAMPLE15_Peak2_fit)\n",
    "SAMPLE16_Peak2_Selected_Intensities = clean_intensities(processed_df=SAMPLE16_Peak2_fit)\n",
    "SAMPLE17_Peak2_Selected_Intensities = clean_intensities(processed_df=SAMPLE17_Peak2_fit)\n",
    "\n",
    "SAMPLE1_Peak3_Selected_Intensities = clean_intensities(processed_df=SAMPLE1_Peak3_fit)\n",
    "SAMPLE2_Peak3_Selected_Intensities = clean_intensities(processed_df=SAMPLE2_Peak3_fit)\n",
    "SAMPLE3_Peak3_Selected_Intensities = clean_intensities(processed_df=SAMPLE3_Peak3_fit)\n",
    "SAMPLE4_Peak3_Selected_Intensities = clean_intensities(processed_df=SAMPLE4_Peak3_fit)\n",
    "SAMPLE5_Peak3_Selected_Intensities = clean_intensities(processed_df=SAMPLE5_Peak3_fit)\n",
    "SAMPLE6_Peak3_Selected_Intensities = clean_intensities(processed_df=SAMPLE6_Peak3_fit)\n",
    "SAMPLE7_Peak3_Selected_Intensities = clean_intensities(processed_df=SAMPLE7_Peak3_fit)\n",
    "SAMPLE8_Peak3_Selected_Intensities = clean_intensities(processed_df=SAMPLE8_Peak3_fit)\n",
    "SAMPLE9_Peak3_Selected_Intensities = clean_intensities(processed_df=SAMPLE9_Peak3_fit)\n",
    "SAMPLE10_Peak3_Selected_Intensities = clean_intensities(processed_df=SAMPLE10_Peak3_fit)\n",
    "SAMPLE11_Peak3_Selected_Intensities = clean_intensities(processed_df=SAMPLE11_Peak3_fit)\n",
    "SAMPLE12_Peak3_Selected_Intensities = clean_intensities(processed_df=SAMPLE12_Peak3_fit)\n",
    "SAMPLE13_Peak3_Selected_Intensities = clean_intensities(processed_df=SAMPLE13_Peak3_fit)\n",
    "SAMPLE14_Peak3_Selected_Intensities = clean_intensities(processed_df=SAMPLE14_Peak3_fit)\n",
    "SAMPLE15_Peak3_Selected_Intensities = clean_intensities(processed_df=SAMPLE15_Peak3_fit)\n",
    "SAMPLE16_Peak3_Selected_Intensities = clean_intensities(processed_df=SAMPLE16_Peak3_fit)\n",
    "SAMPLE17_Peak3_Selected_Intensities = clean_intensities(processed_df=SAMPLE17_Peak3_fit)\n",
    "\n",
    "SAMPLE1_Peak4_Selected_Intensities = clean_intensities(processed_df=SAMPLE1_Peak4_fit)\n",
    "SAMPLE2_Peak4_Selected_Intensities = clean_intensities(processed_df=SAMPLE2_Peak4_fit)\n",
    "SAMPLE3_Peak4_Selected_Intensities = clean_intensities(processed_df=SAMPLE3_Peak4_fit)\n",
    "SAMPLE4_Peak4_Selected_Intensities = clean_intensities(processed_df=SAMPLE4_Peak4_fit)\n",
    "SAMPLE5_Peak4_Selected_Intensities = clean_intensities(processed_df=SAMPLE5_Peak4_fit)\n",
    "SAMPLE6_Peak4_Selected_Intensities = clean_intensities(processed_df=SAMPLE6_Peak4_fit)\n",
    "SAMPLE7_Peak4_Selected_Intensities = clean_intensities(processed_df=SAMPLE7_Peak4_fit)\n",
    "SAMPLE8_Peak4_Selected_Intensities = clean_intensities(processed_df=SAMPLE8_Peak4_fit)\n",
    "SAMPLE9_Peak4_Selected_Intensities = clean_intensities(processed_df=SAMPLE9_Peak4_fit)\n",
    "SAMPLE10_Peak4_Selected_Intensities = clean_intensities(processed_df=SAMPLE10_Peak4_fit)\n",
    "SAMPLE11_Peak4_Selected_Intensities = clean_intensities(processed_df=SAMPLE11_Peak4_fit)\n",
    "SAMPLE12_Peak4_Selected_Intensities = clean_intensities(processed_df=SAMPLE12_Peak4_fit)\n",
    "SAMPLE13_Peak4_Selected_Intensities = clean_intensities(processed_df=SAMPLE13_Peak4_fit)\n",
    "SAMPLE14_Peak4_Selected_Intensities = clean_intensities(processed_df=SAMPLE14_Peak4_fit)\n",
    "SAMPLE15_Peak4_Selected_Intensities = clean_intensities(processed_df=SAMPLE15_Peak4_fit)\n",
    "SAMPLE16_Peak4_Selected_Intensities = clean_intensities(processed_df=SAMPLE16_Peak4_fit)\n",
    "SAMPLE17_Peak4_Selected_Intensities = clean_intensities(processed_df=SAMPLE17_Peak4_fit)\n",
    "\n",
    "SAMPLE1_Peak1_Selected_Intensities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486be826-bcad-4e87-8bc5-09d6eef0dd9d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# SAMPLE1_Peak1_Selected_Intensities_all = clean_intensities_all(processed_df=SAMPLE1_Peak1_fit)\n",
    "# SAMPLE2_Peak1_Selected_Intensities_all = clean_intensities_all(processed_df=SAMPLE2_Peak1_fit)\n",
    "# SAMPLE3_Peak1_Selected_Intensities_all = clean_intensities_all(processed_df=SAMPLE3_Peak1_fit)\n",
    "# SAMPLE4_Peak1_Selected_Intensities_all = clean_intensities_all(processed_df=SAMPLE4_Peak1_fit)\n",
    "# SAMPLE5_Peak1_Selected_Intensities_all = clean_intensities_all(processed_df=SAMPLE5_Peak1_fit)\n",
    "# SAMPLE6_Peak1_Selected_Intensities_all = clean_intensities_all(processed_df=SAMPLE6_Peak1_fit)\n",
    "# SAMPLE7_Peak1_Selected_Intensities_all = clean_intensities_all(processed_df=SAMPLE7_Peak1_fit)\n",
    "# SAMPLE8_Peak1_Selected_Intensities_all = clean_intensities_all(processed_df=SAMPLE8_Peak1_fit)\n",
    "# SAMPLE9_Peak1_Selected_Intensities_all = clean_intensities_all(processed_df=SAMPLE9_Peak1_fit)\n",
    "# SAMPLE10_Peak1_Selected_Intensities_all = clean_intensities_all(processed_df=SAMPLE10_Peak1_fit)\n",
    "# SAMPLE11_Peak1_Selected_Intensities_all = clean_intensities_all(processed_df=SAMPLE11_Peak1_fit)\n",
    "# SAMPLE12_Peak1_Selected_Intensities_all = clean_intensities_all(processed_df=SAMPLE12_Peak1_fit)\n",
    "# SAMPLE13_Peak1_Selected_Intensities_all = clean_intensities_all(processed_df=SAMPLE13_Peak1_fit)\n",
    "# SAMPLE14_Peak1_Selected_Intensities_all = clean_intensities_all(processed_df=SAMPLE14_Peak1_fit)\n",
    "# SAMPLE15_Peak1_Selected_Intensities_all = clean_intensities_all(processed_df=SAMPLE15_Peak1_fit)\n",
    "# SAMPLE16_Peak1_Selected_Intensities_all = clean_intensities_all(processed_df=SAMPLE16_Peak1_fit)\n",
    "# SAMPLE17_Peak1_Selected_Intensities_all = clean_intensities_all(processed_df=SAMPLE17_Peak1_fit)\n",
    "\n",
    "\n",
    "# SAMPLE1_Peak2_Selected_Intensities_all = clean_intensities_all(processed_df=SAMPLE1_Peak2_fit)\n",
    "# SAMPLE2_Peak2_Selected_Intensities_all = clean_intensities_all(processed_df=SAMPLE2_Peak2_fit)\n",
    "# SAMPLE3_Peak2_Selected_Intensities_all = clean_intensities_all(processed_df=SAMPLE3_Peak2_fit)\n",
    "# SAMPLE4_Peak2_Selected_Intensities_all = clean_intensities_all(processed_df=SAMPLE4_Peak2_fit)\n",
    "# SAMPLE5_Peak2_Selected_Intensities_all = clean_intensities_all(processed_df=SAMPLE5_Peak2_fit)\n",
    "# SAMPLE6_Peak2_Selected_Intensities_all = clean_intensities_all(processed_df=SAMPLE6_Peak2_fit)\n",
    "# SAMPLE7_Peak2_Selected_Intensities_all = clean_intensities_all(processed_df=SAMPLE7_Peak2_fit)\n",
    "# SAMPLE8_Peak2_Selected_Intensities_all = clean_intensities_all(processed_df=SAMPLE8_Peak2_fit)\n",
    "# SAMPLE9_Peak2_Selected_Intensities_all = clean_intensities_all(processed_df=SAMPLE9_Peak2_fit)\n",
    "# SAMPLE10_Peak2_Selected_Intensities_all = clean_intensities_all(processed_df=SAMPLE10_Peak2_fit)\n",
    "# SAMPLE11_Peak2_Selected_Intensities_all = clean_intensities_all(processed_df=SAMPLE11_Peak2_fit)\n",
    "# SAMPLE12_Peak2_Selected_Intensities_all = clean_intensities_all(processed_df=SAMPLE12_Peak2_fit)\n",
    "# SAMPLE13_Peak2_Selected_Intensities_all = clean_intensities_all(processed_df=SAMPLE13_Peak2_fit)\n",
    "# SAMPLE14_Peak2_Selected_Intensities_all = clean_intensities_all(processed_df=SAMPLE14_Peak2_fit)\n",
    "# SAMPLE15_Peak2_Selected_Intensities_all = clean_intensities_all(processed_df=SAMPLE15_Peak2_fit)\n",
    "# SAMPLE16_Peak2_Selected_Intensities_all = clean_intensities_all(processed_df=SAMPLE16_Peak2_fit)\n",
    "# SAMPLE17_Peak2_Selected_Intensities_all = clean_intensities_all(processed_df=SAMPLE17_Peak2_fit)\n",
    "\n",
    "# SAMPLE1_Peak3_Selected_Intensities_all = clean_intensities_all(processed_df=SAMPLE1_Peak3_fit)\n",
    "# SAMPLE2_Peak3_Selected_Intensities_all = clean_intensities_all(processed_df=SAMPLE2_Peak3_fit)\n",
    "# SAMPLE3_Peak3_Selected_Intensities_all = clean_intensities_all(processed_df=SAMPLE3_Peak3_fit)\n",
    "# SAMPLE4_Peak3_Selected_Intensities_all = clean_intensities_all(processed_df=SAMPLE4_Peak3_fit)\n",
    "# SAMPLE5_Peak3_Selected_Intensities_all = clean_intensities_all(processed_df=SAMPLE5_Peak3_fit)\n",
    "# SAMPLE6_Peak3_Selected_Intensities_all = clean_intensities_all(processed_df=SAMPLE6_Peak3_fit)\n",
    "# SAMPLE7_Peak3_Selected_Intensities_all = clean_intensities_all(processed_df=SAMPLE7_Peak3_fit)\n",
    "# SAMPLE8_Peak3_Selected_Intensities_all = clean_intensities_all(processed_df=SAMPLE8_Peak3_fit)\n",
    "# SAMPLE9_Peak3_Selected_Intensities_all = clean_intensities_all(processed_df=SAMPLE9_Peak3_fit)\n",
    "# SAMPLE10_Peak3_Selected_Intensities_all = clean_intensities_all(processed_df=SAMPLE10_Peak3_fit)\n",
    "# SAMPLE11_Peak3_Selected_Intensities_all = clean_intensities_all(processed_df=SAMPLE11_Peak3_fit)\n",
    "# SAMPLE12_Peak3_Selected_Intensities_all = clean_intensities_all(processed_df=SAMPLE12_Peak3_fit)\n",
    "# SAMPLE13_Peak3_Selected_Intensities_all = clean_intensities_all(processed_df=SAMPLE13_Peak3_fit)\n",
    "# SAMPLE14_Peak3_Selected_Intensities_all = clean_intensities_all(processed_df=SAMPLE14_Peak3_fit)\n",
    "# SAMPLE15_Peak3_Selected_Intensities_all = clean_intensities_all(processed_df=SAMPLE15_Peak3_fit)\n",
    "# SAMPLE16_Peak3_Selected_Intensities_all = clean_intensities_all(processed_df=SAMPLE16_Peak3_fit)\n",
    "# SAMPLE17_Peak3_Selected_Intensities_all = clean_intensities_all(processed_df=SAMPLE17_Peak3_fit)\n",
    "\n",
    "# SAMPLE1_Peak4_Selected_Intensities_all = clean_intensities_all(processed_df=SAMPLE1_Peak4_fit)\n",
    "# SAMPLE2_Peak4_Selected_Intensities_all = clean_intensities_all(processed_df=SAMPLE2_Peak4_fit)\n",
    "# SAMPLE3_Peak4_Selected_Intensities_all = clean_intensities_all(processed_df=SAMPLE3_Peak4_fit)\n",
    "# SAMPLE4_Peak4_Selected_Intensities_all = clean_intensities_all(processed_df=SAMPLE4_Peak4_fit)\n",
    "# SAMPLE5_Peak4_Selected_Intensities_all = clean_intensities_all(processed_df=SAMPLE5_Peak4_fit)\n",
    "# SAMPLE6_Peak4_Selected_Intensities_all = clean_intensities_all(processed_df=SAMPLE6_Peak4_fit)\n",
    "# SAMPLE7_Peak4_Selected_Intensities_all = clean_intensities_all(processed_df=SAMPLE7_Peak4_fit)\n",
    "# SAMPLE8_Peak4_Selected_Intensities_all = clean_intensities_all(processed_df=SAMPLE8_Peak4_fit)\n",
    "# SAMPLE9_Peak4_Selected_Intensities_all = clean_intensities_all(processed_df=SAMPLE9_Peak4_fit)\n",
    "# SAMPLE10_Peak4_Selected_Intensities_all = clean_intensities_all(processed_df=SAMPLE10_Peak4_fit)\n",
    "# SAMPLE11_Peak4_Selected_Intensities_all = clean_intensities_all(processed_df=SAMPLE11_Peak4_fit)\n",
    "# SAMPLE12_Peak4_Selected_Intensities_all = clean_intensities_all(processed_df=SAMPLE12_Peak4_fit)\n",
    "# SAMPLE13_Peak4_Selected_Intensities_all = clean_intensities_all(processed_df=SAMPLE13_Peak4_fit)\n",
    "# SAMPLE14_Peak4_Selected_Intensities_all = clean_intensities_all(processed_df=SAMPLE14_Peak4_fit)\n",
    "# SAMPLE15_Peak4_Selected_Intensities_all = clean_intensities_all(processed_df=SAMPLE15_Peak4_fit)\n",
    "# SAMPLE16_Peak4_Selected_Intensities_all = clean_intensities_all(processed_df=SAMPLE16_Peak4_fit)\n",
    "# SAMPLE17_Peak4_Selected_Intensities_all = clean_intensities_all(processed_df=SAMPLE17_Peak4_fit)\n",
    "\n",
    "# SAMPLE1_Peak1_Selected_Intensities_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac273ac-b736-4151-9720-b4259db9ec7e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# intensity_data_all = {\n",
    "#     'Peak1': [\n",
    "#         SAMPLE1_Peak1_Selected_Intensities_all,\n",
    "#         SAMPLE2_Peak1_Selected_Intensities_all,\n",
    "#         SAMPLE3_Peak1_Selected_Intensities_all,\n",
    "#         SAMPLE4_Peak1_Selected_Intensities_all,\n",
    "#         SAMPLE5_Peak1_Selected_Intensities_all,\n",
    "#         SAMPLE6_Peak1_Selected_Intensities_all,\n",
    "#         SAMPLE7_Peak1_Selected_Intensities_all,\n",
    "#         SAMPLE8_Peak1_Selected_Intensities_all,\n",
    "#         SAMPLE9_Peak1_Selected_Intensities_all,\n",
    "#         SAMPLE10_Peak1_Selected_Intensities_all,\n",
    "#         SAMPLE11_Peak1_Selected_Intensities_all,\n",
    "#         SAMPLE12_Peak1_Selected_Intensities_all,\n",
    "#         SAMPLE13_Peak1_Selected_Intensities_all,\n",
    "#         SAMPLE14_Peak1_Selected_Intensities_all,\n",
    "#         SAMPLE15_Peak1_Selected_Intensities_all,\n",
    "#         SAMPLE16_Peak1_Selected_Intensities_all,\n",
    "#         SAMPLE17_Peak1_Selected_Intensities_all\n",
    "#     ],\n",
    "    \n",
    "#     'Peak2': [\n",
    "#         SAMPLE1_Peak2_Selected_Intensities_all,\n",
    "#         SAMPLE2_Peak2_Selected_Intensities_all,\n",
    "#         SAMPLE3_Peak2_Selected_Intensities_all,\n",
    "#         SAMPLE4_Peak2_Selected_Intensities_all,\n",
    "#         SAMPLE5_Peak2_Selected_Intensities_all,\n",
    "#         SAMPLE6_Peak2_Selected_Intensities_all,\n",
    "#         SAMPLE7_Peak2_Selected_Intensities_all,\n",
    "#         SAMPLE8_Peak2_Selected_Intensities_all,\n",
    "#         SAMPLE9_Peak2_Selected_Intensities_all,\n",
    "#         SAMPLE10_Peak2_Selected_Intensities_all,\n",
    "#         SAMPLE11_Peak2_Selected_Intensities_all,\n",
    "#         SAMPLE12_Peak2_Selected_Intensities_all,\n",
    "#         SAMPLE13_Peak2_Selected_Intensities_all,\n",
    "#         SAMPLE14_Peak2_Selected_Intensities_all,\n",
    "#         SAMPLE15_Peak2_Selected_Intensities_all,\n",
    "#         SAMPLE16_Peak2_Selected_Intensities_all,\n",
    "#         SAMPLE17_Peak2_Selected_Intensities_all\n",
    "#     ],\n",
    "    \n",
    "#     'Peak3': [\n",
    "#         SAMPLE1_Peak3_Selected_Intensities_all,\n",
    "#         SAMPLE2_Peak3_Selected_Intensities_all,\n",
    "#         SAMPLE3_Peak3_Selected_Intensities_all,\n",
    "#         SAMPLE4_Peak3_Selected_Intensities_all,\n",
    "#         SAMPLE5_Peak3_Selected_Intensities_all,\n",
    "#         SAMPLE6_Peak3_Selected_Intensities_all,\n",
    "#         SAMPLE7_Peak3_Selected_Intensities_all,\n",
    "#         SAMPLE8_Peak3_Selected_Intensities_all,\n",
    "#         SAMPLE9_Peak3_Selected_Intensities_all,\n",
    "#         SAMPLE10_Peak3_Selected_Intensities_all,\n",
    "#         SAMPLE11_Peak3_Selected_Intensities_all,\n",
    "#         SAMPLE12_Peak3_Selected_Intensities_all,\n",
    "#         SAMPLE13_Peak3_Selected_Intensities_all,\n",
    "#         SAMPLE14_Peak3_Selected_Intensities_all,\n",
    "#         SAMPLE15_Peak3_Selected_Intensities_all,\n",
    "#         SAMPLE16_Peak3_Selected_Intensities_all,\n",
    "#         SAMPLE17_Peak3_Selected_Intensities_all\n",
    "#     ],\n",
    "    \n",
    "#     'Peak4': [\n",
    "#         SAMPLE1_Peak4_Selected_Intensities_all,\n",
    "#         SAMPLE2_Peak4_Selected_Intensities_all,\n",
    "#         SAMPLE3_Peak4_Selected_Intensities_all,\n",
    "#         SAMPLE4_Peak4_Selected_Intensities_all,\n",
    "#         SAMPLE5_Peak4_Selected_Intensities_all,\n",
    "#         SAMPLE6_Peak4_Selected_Intensities_all,\n",
    "#         SAMPLE7_Peak4_Selected_Intensities_all,\n",
    "#         SAMPLE8_Peak4_Selected_Intensities_all,\n",
    "#         SAMPLE9_Peak4_Selected_Intensities_all,\n",
    "#         SAMPLE10_Peak4_Selected_Intensities_all,\n",
    "#         SAMPLE11_Peak4_Selected_Intensities_all,\n",
    "#         SAMPLE12_Peak4_Selected_Intensities_all,\n",
    "#         SAMPLE13_Peak4_Selected_Intensities_all,\n",
    "#         SAMPLE14_Peak4_Selected_Intensities_all,\n",
    "#         SAMPLE15_Peak4_Selected_Intensities_all,\n",
    "#         SAMPLE16_Peak4_Selected_Intensities_all,\n",
    "#         SAMPLE17_Peak4_Selected_Intensities_all\n",
    "#     ],\n",
    "    \n",
    "#     'Mn_concentration': [\n",
    "#         1.9, 1.8, 2.3, 1.2, 1.9, 2.3, 0.1, 1.1,\n",
    "#         0.571, 0.489, 0.622, 0.000, 0.347, 0.428, 0.473, 0.515, 0.208\n",
    "#     ],\n",
    "\n",
    "#     'C_concentration':  [\n",
    "#     0.15, 0.15, 0.16, 0.23, 0.21, 0.07, 0.003, 0.06, \n",
    "#     0.1222, 0.0847, 0.2990, 0.0032, 0.2734, 0.8027, 0.6353, 0.6632, 0.0943\n",
    "#     ],\n",
    "\n",
    "#     'Si_concentration':  [\n",
    "#     0.3, 0.3, 0.5, 0.2, 1.8, 0.2, 0.0, 0.4,\n",
    "#     0.13, 0.10, 0.32, 0.00, 0.09, 0.67, 0.58, 0.33, 0.00\n",
    "#     ],\n",
    "\n",
    "#     'Al_concentration':  [\n",
    "#     0.04, 0.04, 0.69, 0.03, 0.04, 0.04, 0.02, 0.04,\n",
    "#     0.280, 0.413, 3.430, 0.000, 0.000, 0.775, 1.995, 0.029, 0.000\n",
    "#     ],\n",
    "\n",
    "#     'Cr_concentration':  [\n",
    "#     0.41, 0.42, 0.70, 0.14, 0.05, 0.60, 0.03, 0.03,\n",
    "#     0.550, 0.020, 2.150, 0.000, 0.021, 0.591, 0.184, 0.194, 0.007\n",
    "#     ],\n",
    "\n",
    "#     'Cu_concentration':  [\n",
    "#     0.02, 0.02, 0.03, 0.02, 0.03, 0.02, 0.01, 0.01,\n",
    "#     0.036, 0.101, 0.049, 0.006, 0.044, 0.220, 0.430, 1.585, 0.067\n",
    "#     ],\n",
    "#     'Ni_concentration':  [\n",
    "#     0.030, 0.037, 0.048, 0.020, 0.037, 0.040, 0.020, 0.010,\n",
    "#     0.000, 0.000, 0.000, 0.000, 0.047, 0.054, 0.075, 0.077, 0.023\n",
    "#     ],\n",
    "#     'Nb_concentration':  [\n",
    "#     0.006, 0.006, 0.032, 0.001, 0.056, 0.001, 0.001, 0.046,\n",
    "#     0.0191, 0.0000, 0.0337, 0.0000, 0.0021, 0.0169, 0.0136, 0.0000, 0.0030\n",
    "#     ],\n",
    "#     'Mo_concentration':  [\n",
    "#     0.101, 0.105, 0.023, 0.000, 0.006, 0.020, 0.005, 0.002,\n",
    "#     0.000, 0.007, 0.126, 0.007, 0.008, 0.010, 0.005, 0.021, 0.007\n",
    "#     ],\n",
    "#     'Ti_concentration':  [\n",
    "#     0.044, 0.037, 0.025, 0.034, 0.014, 0.051, 0.068, 0.028,\n",
    "#     0.0353, 0.0086, 0.0502, 0.0000, 0.0000, 0.0181, 0.0158, 0.0067, 0.0000\n",
    "#     ]\n",
    "# }\n",
    "\n",
    "\n",
    "# # Creating the DataFrame\n",
    "# df_all = pd.DataFrame(intensity_data_all)\n",
    "\n",
    "# # Print the DataFrame\n",
    "# print(df_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c45b141-b3d5-44d6-a046-13af8e92b404",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# df_all_np = df_all.to_numpy()\n",
    "# df_all_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532fec55-b5c8-481f-858d-c5d0cf4f7e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "intensity_data = {\n",
    "    'Peak1': [\n",
    "        SAMPLE1_Peak1_Selected_Intensities,\n",
    "        SAMPLE2_Peak1_Selected_Intensities,\n",
    "        SAMPLE3_Peak1_Selected_Intensities,\n",
    "        SAMPLE4_Peak1_Selected_Intensities,\n",
    "        SAMPLE5_Peak1_Selected_Intensities,\n",
    "        SAMPLE6_Peak1_Selected_Intensities,\n",
    "        SAMPLE7_Peak1_Selected_Intensities,\n",
    "        SAMPLE8_Peak1_Selected_Intensities,\n",
    "        SAMPLE9_Peak1_Selected_Intensities,\n",
    "        SAMPLE10_Peak1_Selected_Intensities,\n",
    "        SAMPLE11_Peak1_Selected_Intensities,\n",
    "        SAMPLE12_Peak1_Selected_Intensities,\n",
    "        SAMPLE13_Peak1_Selected_Intensities,\n",
    "        SAMPLE14_Peak1_Selected_Intensities,\n",
    "        SAMPLE15_Peak1_Selected_Intensities,\n",
    "        SAMPLE16_Peak1_Selected_Intensities,\n",
    "        SAMPLE17_Peak1_Selected_Intensities\n",
    "    ],\n",
    "    \n",
    "    'Peak2': [\n",
    "        SAMPLE1_Peak2_Selected_Intensities,\n",
    "        SAMPLE2_Peak2_Selected_Intensities,\n",
    "        SAMPLE3_Peak2_Selected_Intensities,\n",
    "        SAMPLE4_Peak2_Selected_Intensities,\n",
    "        SAMPLE5_Peak2_Selected_Intensities,\n",
    "        SAMPLE6_Peak2_Selected_Intensities,\n",
    "        SAMPLE7_Peak2_Selected_Intensities,\n",
    "        SAMPLE8_Peak2_Selected_Intensities,\n",
    "        SAMPLE9_Peak2_Selected_Intensities,\n",
    "        SAMPLE10_Peak2_Selected_Intensities,\n",
    "        SAMPLE11_Peak2_Selected_Intensities,\n",
    "        SAMPLE12_Peak2_Selected_Intensities,\n",
    "        SAMPLE13_Peak2_Selected_Intensities,\n",
    "        SAMPLE14_Peak2_Selected_Intensities,\n",
    "        SAMPLE15_Peak2_Selected_Intensities,\n",
    "        SAMPLE16_Peak2_Selected_Intensities,\n",
    "        SAMPLE17_Peak2_Selected_Intensities\n",
    "    ],\n",
    "    \n",
    "    'Peak3': [\n",
    "        SAMPLE1_Peak3_Selected_Intensities,\n",
    "        SAMPLE2_Peak3_Selected_Intensities,\n",
    "        SAMPLE3_Peak3_Selected_Intensities,\n",
    "        SAMPLE4_Peak3_Selected_Intensities,\n",
    "        SAMPLE5_Peak3_Selected_Intensities,\n",
    "        SAMPLE6_Peak3_Selected_Intensities,\n",
    "        SAMPLE7_Peak3_Selected_Intensities,\n",
    "        SAMPLE8_Peak3_Selected_Intensities,\n",
    "        SAMPLE9_Peak3_Selected_Intensities,\n",
    "        SAMPLE10_Peak3_Selected_Intensities,\n",
    "        SAMPLE11_Peak3_Selected_Intensities,\n",
    "        SAMPLE12_Peak3_Selected_Intensities,\n",
    "        SAMPLE13_Peak3_Selected_Intensities,\n",
    "        SAMPLE14_Peak3_Selected_Intensities,\n",
    "        SAMPLE15_Peak3_Selected_Intensities,\n",
    "        SAMPLE16_Peak3_Selected_Intensities,\n",
    "        SAMPLE17_Peak3_Selected_Intensities\n",
    "    ],\n",
    "    \n",
    "    'Peak4': [\n",
    "        SAMPLE1_Peak4_Selected_Intensities,\n",
    "        SAMPLE2_Peak4_Selected_Intensities,\n",
    "        SAMPLE3_Peak4_Selected_Intensities,\n",
    "        SAMPLE4_Peak4_Selected_Intensities,\n",
    "        SAMPLE5_Peak4_Selected_Intensities,\n",
    "        SAMPLE6_Peak4_Selected_Intensities,\n",
    "        SAMPLE7_Peak4_Selected_Intensities,\n",
    "        SAMPLE8_Peak4_Selected_Intensities,\n",
    "        SAMPLE9_Peak4_Selected_Intensities,\n",
    "        SAMPLE10_Peak4_Selected_Intensities,\n",
    "        SAMPLE11_Peak4_Selected_Intensities,\n",
    "        SAMPLE12_Peak4_Selected_Intensities,\n",
    "        SAMPLE13_Peak4_Selected_Intensities,\n",
    "        SAMPLE14_Peak4_Selected_Intensities,\n",
    "        SAMPLE15_Peak4_Selected_Intensities,\n",
    "        SAMPLE16_Peak4_Selected_Intensities,\n",
    "        SAMPLE17_Peak4_Selected_Intensities\n",
    "    ],\n",
    "    \n",
    "    'Mn_concentration': [\n",
    "        1.9, 1.8, 2.3, 1.2, 1.9, 2.3, 0.1, 1.1,\n",
    "        0.571, 0.489, 0.622, 0.000, 0.347, 0.428, 0.473, 0.515, 0.208\n",
    "    ],\n",
    "\n",
    "    'C_concentration':  [\n",
    "    0.15, 0.15, 0.16, 0.23, 0.21, 0.07, 0.003, 0.06, \n",
    "    0.1222, 0.0847, 0.2990, 0.0032, 0.2734, 0.8027, 0.6353, 0.6632, 0.0943\n",
    "    ],\n",
    "\n",
    "    'Si_concentration':  [\n",
    "    0.3, 0.3, 0.5, 0.2, 1.8, 0.2, 0.0, 0.4,\n",
    "    0.13, 0.10, 0.32, 0.00, 0.09, 0.67, 0.58, 0.33, 0.00\n",
    "    ],\n",
    "\n",
    "    'Al_concentration':  [\n",
    "    0.04, 0.04, 0.69, 0.03, 0.04, 0.04, 0.02, 0.04,\n",
    "    0.280, 0.413, 3.430, 0.000, 0.000, 0.775, 1.995, 0.029, 0.000\n",
    "    ],\n",
    "\n",
    "    'Cr_concentration':  [\n",
    "    0.41, 0.42, 0.70, 0.14, 0.05, 0.60, 0.03, 0.03,\n",
    "    0.550, 0.020, 2.150, 0.000, 0.021, 0.591, 0.184, 0.194, 0.007\n",
    "    ],\n",
    "\n",
    "    'Cu_concentration':  [\n",
    "    0.02, 0.02, 0.03, 0.02, 0.03, 0.02, 0.01, 0.01,\n",
    "    0.036, 0.101, 0.049, 0.006, 0.044, 0.220, 0.430, 1.585, 0.067\n",
    "    ],\n",
    "    'Ni_concentration':  [\n",
    "    0.030, 0.037, 0.048, 0.020, 0.037, 0.040, 0.020, 0.010,\n",
    "    0.000, 0.000, 0.000, 0.000, 0.047, 0.054, 0.075, 0.077, 0.023\n",
    "    ],\n",
    "    'Nb_concentration':  [\n",
    "    0.006, 0.006, 0.032, 0.001, 0.056, 0.001, 0.001, 0.046,\n",
    "    0.0191, 0.0000, 0.0337, 0.0000, 0.0021, 0.0169, 0.0136, 0.0000, 0.0030\n",
    "    ],\n",
    "    'Mo_concentration':  [\n",
    "    0.101, 0.105, 0.023, 0.000, 0.006, 0.020, 0.005, 0.002,\n",
    "    0.000, 0.007, 0.126, 0.007, 0.008, 0.010, 0.005, 0.021, 0.007\n",
    "    ],\n",
    "    'Ti_concentration':  [\n",
    "    0.044, 0.037, 0.025, 0.034, 0.014, 0.051, 0.068, 0.028,\n",
    "    0.0353, 0.0086, 0.0502, 0.0000, 0.0000, 0.0181, 0.0158, 0.0067, 0.0000\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "# Creating the DataFrame\n",
    "df = pd.DataFrame(intensity_data)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ae818b-c6bf-4c7e-9664-a90da21bd3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_np = df.to_numpy()\n",
    "df_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a36c47-cffe-464d-8c2f-4badef73b378",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rows_batch1 = [0,1,3,5,6,7]  #SAMPLENAME - 1 , for sample 9 put 9 , for sample 8 put 7\n",
    "test_rows = [0,1]\n",
    "\n",
    "#Enter Rows = [Sample number -1 ]\n",
    "\n",
    "#8,9,12,13,14,15 - Silicon\n",
    "#0,1,2,3,4,5,7,8,9,10,12,13,14,15,16 - Manganese\n",
    "#0,1,3,5,6,7 - Titanium\n",
    "\n",
    "\n",
    "rows_locov = [0,1,3,5,6,7]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2abbffc-c581-4275-85c6-cf94d7ed3649",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df.loc[train_rows_batch1 , ['Peak2','Peak3','Peak1']]\n",
    "y_train = df.loc[train_rows_batch1, 'Ti_concentration']\n",
    "\n",
    "X_train.shape , y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ad7cdb-44f7-4673-8e71-89e9d8568cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_test = df.loc[test_rows , ['Peak2','Peak3','Peak1']]\n",
    "y_test = df.loc[test_rows, 'Ti_concentration']\n",
    "\n",
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f31799a-774f-4d51-a6fe-a4bd23b50900",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_locov = df.loc[rows_locov , ['Peak2','Peak3','Peak1']]\n",
    "y_locov = df.loc[rows_locov, 'Ti_concentration']\n",
    "\n",
    "X_locov.shape , y_locov.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a2f24c-7ed2-49d4-a1a1-819dde7f7134",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# X_test_all = df_all.loc[test_rows , ['Peak1','Peak2' ]]\n",
    "# y_test_all = df_all.loc[test_rows, 'Si_concentration']\n",
    "\n",
    "# X_test_all.shape, y_test_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da3ecc5-0bb4-4a82-a0e4-59351b128a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train , y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87901d20-ef31-4cab-8c86-166e7ee18e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test , y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71408943-0600-48b1-bd0f-ca41023b6a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_locov , y_locov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d7aa23-07f9-446d-8199-1a9c55f4fea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test_all , y_test_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089a1311-2364-4062-893a-57bfbf25bcde",
   "metadata": {},
   "source": [
    "At the last , now we have list we have selected intensities for each sample , also we have the concentration values per element of each sample , we can now do the regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5562be64-537c-46e1-9b1e-ca6419b1409b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score , mean_squared_error\n",
    "\n",
    "# Initialize the Linear Regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# model_scor = model.score(X_train,y_train)\n",
    "# model_rmse = np.sqrt(mean_squared_error(X_train, y_train))\n",
    "# print('calibration r2: ',model_scor)\n",
    "# print('calibration rmse: ',model_rmse)\n",
    "\n",
    "conc_prediction = model.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "# r2 = r2_score(y_test , conc_prediction )\n",
    "# rmse = np.sqrt(mean_squared_error(y_test, conc_prediction))\n",
    "\n",
    "# print('validation r2: ',r2)\n",
    "# print('validation rmse: ',rmse)\n",
    "\n",
    "\n",
    "\n",
    "# Get the model coefficients and intercept\n",
    "coefficients = model.coef_\n",
    "intercept = model.intercept_\n",
    "\n",
    "\n",
    "# Print the model coefficients and intercept\n",
    "print(\"Model Coefficients:\", coefficients)\n",
    "print(\"Model Intercept:\", intercept)\n",
    "\n",
    "# Create the equation string\n",
    "coeff_str = ' + '.join([f'{coef:.4f}x{i+1}' for i, coef in enumerate(coefficients)])\n",
    "equation = f'Y =  {coeff_str} + {intercept:.4f}'\n",
    "\n",
    "\n",
    "print (equation)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, conc_prediction, color='black', label='Predicted Conc vs Actual Conc')\n",
    "plt.plot([0,0.07], [0,0.07], color='red', linestyle='--')\n",
    "title_6 = (Element_name) + ' BATCH 2 - MLR PEAK AREA' +''\n",
    "plt.title(title_6)\n",
    "plt.xlabel('Actual concentration (%)')\n",
    "plt.ylabel('Predicted concentration (%)')\n",
    "# plt.text(0.92,0.05, f'R^2 = {r2:.4f}\\nRMSE = {rmse:.4f}\\n{equation}', transform=plt.gcf().transFigure, fontsize=10)\n",
    "plt.savefig((title_6)+ \".png\" ,dpi = 1200 , bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5835a17b-c882-4bb5-a8f0-28c13af000d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "# Prepare the feature matrix and target vector\n",
    "X = (X_locov).to_numpy()\n",
    "y = (y_locov).to_numpy()\n",
    "\n",
    "\n",
    "\n",
    "# Initialize lists to store results\n",
    "predictions = []\n",
    "true_values = []\n",
    "# intercepts =[]\n",
    "# slopes = []\n",
    "\n",
    "# Perform LOCOV\n",
    "for i in range(len(X_locov)):\n",
    "    # Define training and validation sets\n",
    "    X_train = np.delete(X, i, axis=0)\n",
    "    y_train = np.delete(y, i, axis=0)\n",
    "    X_val = X[i].reshape(1,-1)\n",
    "    y_val = y[i]\n",
    "    \n",
    "    # Train the model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_val).reshape(1,-1)\n",
    "    # Store results\n",
    "    predictions.append(y_pred[0]) #extract the value from the array\n",
    "    true_values.append(y_val)\n",
    "\n",
    "    # Extract and store coefficients\n",
    "    # intercepts.append(model.intercept_[0])\n",
    "    # slopes.append(model.coef_[0][0])\n",
    "\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays for evaluation\n",
    "predictions = np.array(predictions)\n",
    "true_values = np.array(true_values)\n",
    "\n",
    "print (true_values)\n",
    "print (predictions)\n",
    "\n",
    "\n",
    "# Calculate metrics\n",
    "mse = mean_squared_error(true_values, predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(true_values, predictions)\n",
    "\n",
    "# print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "print(f\"R-squared (R): {r2:.4f}\")\n",
    "\n",
    "# Print the average coefficients\n",
    "# average_intercept = np.mean(intercepts)\n",
    "# average_slope = np.mean(slopes)\n",
    "# print(f\"Average Intercept: {average_intercept:.4f}\")\n",
    "# print(f\"Average Slope: {average_slope:.4f}\")\n",
    "\n",
    "\n",
    "# Plot true vs. predicted values\n",
    "plt1.figure(figsize=(10, 6))\n",
    "plt1.scatter(true_values, predictions, color='blue', label='Predicted vs True')\n",
    "plt1.plot([min(true_values), max(true_values)], [min(true_values), max(true_values)], color='red', linestyle='--', label='Perfect Fit')\n",
    "plt1.xlabel('Real Concentration %')\n",
    "plt1.ylabel('Predicted Concentration %')\n",
    "plt1.title('True vs Predicted Concentration %')\n",
    "plt1.legend()\n",
    "plt1.grid(True)\n",
    "plt1.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c83dfc7-d3ac-431f-a623-7ec78861d433",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "# Prepare the feature matrix and target vector\n",
    "X = X_locov.to_numpy()\n",
    "y = y_locov.to_numpy()\n",
    "\n",
    "\n",
    "# Parameters\n",
    "P = 2  # Number of samples to leave out for validation\n",
    "\n",
    "# Initialize lists to store results\n",
    "predictions = []\n",
    "true_values = []\n",
    "r2_average = []\n",
    "rmse_average = []\n",
    "\n",
    "# Generate all possible combinations of indices to leave out\n",
    "combinations = list(itertools.combinations(range(len(y)), P))\n",
    "\n",
    "\n",
    "# Perform LPOCV\n",
    "for comb in combinations:\n",
    "    # Define training and validation sets\n",
    "    val_indices = list(comb)\n",
    "    train_indices = [i for i in range(len(X)) if i not in val_indices]\n",
    "    print (val_indices)\n",
    "    \n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    X_val = X[val_indices]\n",
    "    y_val = y[val_indices]\n",
    "    \n",
    "    # Train the model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_val)\n",
    "\n",
    "    r2_alone = r2_score(y_val ,y_pred)\n",
    "    rmse_alone = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "\n",
    "    r2_average.append(r2_alone)\n",
    "    rmse_average.append(rmse_alone)\n",
    "    \n",
    "    # Store results\n",
    "    predictions.extend(y_pred.flatten())  # Flatten and append to list\n",
    "    true_values.extend(y_val.flatten())   # Flatten and append to list\n",
    "\n",
    "# Convert lists to numpy arrays for evaluation\n",
    "predictions = np.array(predictions)\n",
    "true_values = np.array(true_values)\n",
    "\n",
    "# Calculate metrics\n",
    "mse = mean_squared_error(true_values, predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(true_values, predictions)\n",
    "\n",
    "# print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "print(f\"R-squared (R): {r2:.4f}\")\n",
    "\n",
    "# #r2 and rmse average \n",
    "\n",
    "# r2_avg = sum(r2_average) / len(r2_average)\n",
    "print(r2_average)\n",
    "\n",
    "# rmse_avg = sum(rmse_average) / len(rmse_average)\n",
    "print(rmse_average)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Plot true vs. predicted values\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(true_values, predictions, color='black', label='Predicted vs True')\n",
    "plt.plot([min(true_values), max(true_values)], [min(true_values), max(true_values)], color='red', linestyle='--', label='Perfect Fit')\n",
    "plt.xlabel('Actual Concentration (%)')\n",
    "plt.ylabel('Predicted Concentration (%)')\n",
    "plt.title(title_6)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
